{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Notebook is for analyzing uncertainty of SpeCollate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from configparser import ConfigParser\n",
    "from os.path import join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.special import iv, logsumexp\n",
    "# from spherecluster import VonMisesFisherMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../src\")\n",
    "from src.snapconfig import config\n",
    "config.PARAM_PATH = \"../config.ini\"\n",
    "\n",
    "import run_train as main\n",
    "from src.snaptrain import dataset, trainmodel\n",
    "from src.snaputils import simulatespectra as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary config func. Original one in the project.\n",
    "class config:\n",
    "    \"\"\"Define constants\"\"\"\n",
    "\n",
    "    AAMass = OrderedDict(\n",
    "        [\n",
    "            (\"A\", 71.037114),\n",
    "            (\"C\", 103.009185),\n",
    "            (\"D\", 115.026943),\n",
    "            (\"E\", 129.042593),\n",
    "            (\"F\", 147.068414),\n",
    "            (\"G\", 57.021464),\n",
    "            (\"H\", 137.058912),\n",
    "            (\"I\", 113.084064),\n",
    "            (\"K\", 128.094963),\n",
    "            (\"L\", 113.084064),\n",
    "            (\"M\", 131.040485),\n",
    "            (\"N\", 114.042927),\n",
    "            (\"P\", 97.052764),\n",
    "            (\"Q\", 128.058578),\n",
    "            (\"R\", 156.101111),\n",
    "            (\"S\", 87.032028),\n",
    "            (\"T\", 101.047679),\n",
    "            (\"V\", 99.068414),\n",
    "            (\"W\", 186.079313),\n",
    "            (\"Y\", 163.0633),\n",
    "            (\"p\", 79.9663),\n",
    "            (\"o\", 15.994915),\n",
    "            (\"h\", 0.9840),\n",
    "            (\"c\", 57.02146),\n",
    "            (\"a\", 42.0106),\n",
    "            (\"r\", -17.026549),\n",
    "            (\"y\", 43.00581),\n",
    "            (\"d\", -18.010565),\n",
    "            (\"t\", 26.02),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    aas = [\"_PAD\"] + list(AAMass.keys())\n",
    "    aa2idx = {a: i for i, a in enumerate(aas)}\n",
    "\n",
    "    ModMass = {\n",
    "        \"Oxidation\": 15.994915,\n",
    "        \"CAM\": 57.02146,\n",
    "        \"Carbamidomethyl\": 57.02146,\n",
    "        \"ICAT_light\": 227.12,\n",
    "        \"ICAT_heavy\": 236.12,\n",
    "        \"AB_old_ICATd0\": 442.20,\n",
    "        \"AB_old_ICATd8\": 450.20,\n",
    "        \"Acetyl\": 42.0106,\n",
    "        \"Deamidation\": 0.9840,\n",
    "        \"Pyro-cmC\": -17.026549,\n",
    "        \"Pyro-glu\": -17.026549,\n",
    "        \"Pyro_glu\": -18.010565,\n",
    "        \"Amide\": -0.984016,\n",
    "        \"Phospho\": 79.9663,\n",
    "        \"Methyl\": 14.0157,\n",
    "        \"Carbamyl\": 43.00581,\n",
    "    }\n",
    "\n",
    "    ModCHAR = OrderedDict(\n",
    "        [\n",
    "            (\"15.99\", \"o\"),\n",
    "            (\"57.02\", \"c\"),\n",
    "            (\"0.98\", \"h\"),\n",
    "            (\"42.01\", \"a\"),\n",
    "            (\"-17.03\", \"r\"),\n",
    "            (\"79.97\", \"p\"),\n",
    "            (\"43.01\", \"y\"),\n",
    "            (\"-18.01\", \"d\"),\n",
    "            (\"26.02\", \"t\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ModCHAR = {\"15.99\": \"o\", \"0.98\": \"h\", \"57.02\": \"c\", \"42.01\": \"a\", \"-17.03\": \"r\", \"79.97\": \"p\"}\n",
    "    Ignore = [\"U\", \"X\"]\n",
    "    Mods = [\n",
    "        {\"mod_char\": \"p\", \"aas\": [\"S\", \"T\", \"Y\"]}\n",
    "        # {\"mod_char\": \"o\", \"aas\": [\"nt\", \"M\"]}\n",
    "    ]\n",
    "    H2O = 18.010564683\n",
    "    NH3 = 17.031\n",
    "    PROTON = 1.00727647\n",
    "\n",
    "    DEFAULT_PARAM_PATH = os.path.join(os.getcwd(), \"config.ini\")\n",
    "    PARAM_PATH = None\n",
    "    l_config = None\n",
    "\n",
    "    def get_config(section=\"input\", key=None):\n",
    "        \"\"\"Read the configuration parameters and return a dictionary.\"\"\"\n",
    "\n",
    "        # If file path is given use it otherwise use default.\n",
    "        file_path = config.PARAM_PATH if config.PARAM_PATH else config.DEFAULT_PARAM_PATH\n",
    "\n",
    "        # Read config and convert each value to appropriate type.\n",
    "        # Only for the first time.\n",
    "        if not config.l_config:\n",
    "            config.l_config = dict()\n",
    "            config_ = ConfigParser()\n",
    "            assert isinstance(file_path, str)\n",
    "            config_.read(file_path)\n",
    "            for section_ in config_.sections():\n",
    "                config.l_config[section_] = dict()\n",
    "                for key_ in config_[section_]:\n",
    "                    try:\n",
    "                        config.l_config[section_][key_] = ast.literal_eval(config_[section_][key_])\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        config.l_config[section_][key_] = config_[section_][key_]\n",
    "\n",
    "        if section and section in config.l_config:\n",
    "            if key and key in config.l_config[section]:\n",
    "                return config.l_config[section][key]\n",
    "            return config.l_config[section]\n",
    "        return config.l_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "False\n",
      "1024\n",
      "9\n",
      "2\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "config.PARAM_PATH = \"../config.ini\"\n",
    "print(config.get_config(section=\"search\", key=\"charge\"))\n",
    "print(config.get_config(section=\"input\", key=\"use_mods\"))\n",
    "print(config.get_config(section=\"ml\", key=\"batch_size\"))\n",
    "print(config.get_config(section=\"input\", key=\"num_species\"))\n",
    "print(config.get_config(section=\"search\", key=\"num_mods\"))\n",
    "print(config.get_config(section=\"input\", key=\"spec_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppm(val, ppm_val):\n",
    "    return (ppm_val / 1000000) * val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding useless comment\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        output_size=512,\n",
    "        embedding_dim=512,\n",
    "        hidden_lstm_dim=1024,\n",
    "        lstm_layers=2,\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.spec_size = config.get_config(section=\"input\", key=\"spec_size\")\n",
    "        self.spec_size = 80000\n",
    "        self.seq_len = config.get_config(section=\"ml\", key=\"pep_seq_len\")\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_lstm_dim = hidden_lstm_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        ################### Spectra branch ###################\n",
    "        self.linear1_1 = nn.Linear(self.spec_size, 512)\n",
    "        self.linear1_2 = nn.Linear(512, 256)\n",
    "\n",
    "        ################### Peptide branch ###################\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            self.hidden_lstm_dim,\n",
    "            self.lstm_layers,\n",
    "            # dropout=0.5,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.linear2_1 = nn.Linear(self.hidden_lstm_dim * 2, 512)  # 2048, 1024\n",
    "        self.linear2_2 = nn.Linear(512, 256)\n",
    "        do = config.get_config(section=\"ml\", key=\"dropout\")\n",
    "        self.dropout1_1 = nn.Dropout(do)\n",
    "        self.dropout2_1 = nn.Dropout(do)\n",
    "        self.dropout2_2 = nn.Dropout(do)\n",
    "        print(\"dropout: {}\".format(do))\n",
    "\n",
    "    def forward(self, data, data_type=None):\n",
    "        assert not data_type or data_type == \"specs\" or data_type == \"peps\"\n",
    "        res = []\n",
    "        if not data_type or data_type == \"specs\":\n",
    "            specs = data[0].squeeze()\n",
    "\n",
    "            out = F.relu((self.linear1_1(specs.view(-1, self.spec_size))))\n",
    "            out = self.dropout1_1(out)\n",
    "\n",
    "            out_spec = F.relu(self.linear1_2(out))\n",
    "            out_spec = F.normalize(out_spec)\n",
    "            res.append(out_spec)\n",
    "\n",
    "        if not data_type or data_type == \"peps\":\n",
    "            for peps in data[1:3]:\n",
    "                peps = peps.squeeze()\n",
    "                embeds = self.embedding(peps)\n",
    "                hidden = self.init_hidden(len(peps))\n",
    "                hidden = tuple([e.data for e in hidden])\n",
    "                lstm_out, _ = self.lstm(embeds, hidden)\n",
    "                lstm_out = lstm_out[:, -1, :]\n",
    "                out = lstm_out.contiguous().view(-1, self.hidden_lstm_dim * 2)\n",
    "                out = self.dropout2_1(out)\n",
    "\n",
    "                out = F.relu((self.linear2_1(out)))\n",
    "                out = self.dropout2_2(out)\n",
    "\n",
    "                out_pep = F.relu(self.linear2_2(out))\n",
    "                out_pep = F.normalize(out_pep)\n",
    "                res.append(out_pep)\n",
    "        return res\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.lstm_layers * 2, batch_size, self.hidden_lstm_dim).zero_(),\n",
    "            weight.new(self.lstm_layers * 2, batch_size, self.hidden_lstm_dim).zero_(),\n",
    "        )\n",
    "        return hidden\n",
    "\n",
    "    def one_hot_tensor(self, peps):\n",
    "        batch_size = len(peps)\n",
    "        src = torch.zeros((batch_size, self.seq_len), dtype=torch.float16, device=\"cuda\")\n",
    "        src[peps > 0] = 1.0\n",
    "        one_hots = torch.zeros(\n",
    "            (batch_size, self.seq_len, self.vocab_size),\n",
    "            dtype=torch.float16,\n",
    "            device=\"cuda\",\n",
    "        )\n",
    "        one_hots.scatter_(\n",
    "            2,\n",
    "            peps.view(batch_size, self.seq_len, 1),\n",
    "            src.view(batch_size, self.seq_len, 1),\n",
    "        )\n",
    "        one_hots.requires_grad = True\n",
    "        return one_hots\n",
    "\n",
    "    def name(self):\n",
    "        return \"Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 1\n",
    "main.setup(rank, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 0.3\n",
      "Net(\n",
      "  (linear1_1): Linear(in_features=80000, out_features=512, bias=True)\n",
      "  (linear1_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (embedding): Embedding(30, 512)\n",
      "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (linear2_1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (linear2_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout1_1): Dropout(p=0.3, inplace=False)\n",
      "  (dropout2_1): Dropout(p=0.3, inplace=False)\n",
      "  (dropout2_2): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-r2r2r-22.pt\"\n",
    "snap_model = Net(vocab_size=30, embedding_dim=512, hidden_lstm_dim=512, lstm_layers=2).to(rank)\n",
    "snap_model = nn.parallel.DistributedDataParallel(snap_model, device_ids=[rank])\n",
    "snap_model.load_state_dict(torch.load(\"../models/{}\".format(model_name))[\"model_state_dict\"])\n",
    "snap_model = snap_model.module\n",
    "snap_model.eval()\n",
    "print(snap_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lclhome/mtari008/data/deepsnap/nist_massiv_80k_no_ch_graymass-semi\n",
      "Reading train test split listings from pickles.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb Cell 9\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# in_tensor_dir = config.get_config(section='preprocess', key='in_tensor_dir') # for raptor\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m in_tensor_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/lclhome/mtari008/data/deepsnap/nist_massiv_80k_no_ch_graymass-semi\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# for comet\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m (\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     train_peps,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     train_specs,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     train_masses,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     test_peps,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     test_specs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     test_masses,\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m ) \u001b[39m=\u001b[39m main\u001b[39m.\u001b[39;49mread_split_listings(in_tensor_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m np_specs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m spec_path \u001b[39m=\u001b[39m join(in_tensor_dir, \u001b[39m\"\u001b[39m\u001b[39mspectra\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/disk/dragon-storage/homes/mtari008/DeepSNAP/uncertainty_analysis/../run_train.py:241\u001b[0m, in \u001b[0;36mread_split_listings\u001b[0;34m(l_in_tensor_dir)\u001b[0m\n\u001b[1;32m    239\u001b[0m train_masses \u001b[39m=\u001b[39m []\n\u001b[1;32m    240\u001b[0m \u001b[39mfor\u001b[39;00m train_pep \u001b[39min\u001b[39;00m train_peps:\n\u001b[0;32m--> 241\u001b[0m     train_mass \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(re\u001b[39m.\u001b[39;49msearch(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m(\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39md+)-(\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39md+.\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39md+).pep\u001b[39;49m\u001b[39m\"\u001b[39;49m, train_pep)[\u001b[39m2\u001b[39m])\n\u001b[1;32m    242\u001b[0m     train_masses\u001b[39m.\u001b[39mappend(train_mass)\n\u001b[1;32m    244\u001b[0m test_masses \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/deepsnap-sk-0.22/lib/python3.10/re.py:200\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(pattern, string, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[39m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39msearch(string)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Data Embeddings\n",
    "batch_size = config.get_config(section=\"ml\", key=\"batch_size\")\n",
    "# in_tensor_dir = config.get_config(section='preprocess', key='in_tensor_dir') # for raptor\n",
    "in_tensor_dir = \"/lclhome/mtari008/data/deepsnap/nist_massiv_80k_no_ch_graymass-semi\"  # for comet\n",
    "\n",
    "(\n",
    "    train_peps,\n",
    "    train_specs,\n",
    "    train_masses,\n",
    "    test_peps,\n",
    "    test_specs,\n",
    "    test_masses,\n",
    ") = main.read_split_listings(in_tensor_dir)\n",
    "\n",
    "np_specs = []\n",
    "spec_path = join(in_tensor_dir, \"spectra\")\n",
    "\n",
    "# Wrap the outer loop with tqdm\n",
    "for spec_file_list in tqdm(train_specs, desc=\"Outer Loop\"):\n",
    "    for spec_file in spec_file_list:\n",
    "        np_spec = np.load(join(spec_path, spec_file))\n",
    "        np_specs.append(np_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.LabeledSpectra(in_tensor_dir, train_peps, train_specs)\n",
    "test_dataset = dataset.LabeledSpectra(in_tensor_dir, test_peps, test_specs)\n",
    "\n",
    "train_peps_strings, train_dpeps_strings = [], []\n",
    "train_peps_masses, train_dpeps_masses = [], []\n",
    "for train_pep in train_peps:\n",
    "    pep_path = join(in_tensor_dir, \"peptides\", train_pep)\n",
    "    with open(pep_path, \"r\") as f:\n",
    "        pep = f.readlines()[0].strip()\n",
    "        train_peps_masses.append(sim.get_pep_mass(pep))\n",
    "        train_peps_strings.append(pep)\n",
    "        dpep = train_dataset.get_decoy(pep)\n",
    "        if dpep:\n",
    "            train_dpeps_strings.append(dpep)\n",
    "            train_dpeps_masses.append(sim.get_pep_mass(dpep))\n",
    "\n",
    "vocab_size = train_dataset.vocab_size\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    num_workers=8,\n",
    "    collate_fn=main.psm_collate,\n",
    "    # batch_sampler=train_batch_sampler\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    num_workers=8,\n",
    "    collate_fn=main.psm_collate,\n",
    "    # batch_sampler=test_batch_sampler,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "q, p, d, train_spec_labels, train_spec_charges, train_spec_masses = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "print(\"Num batches: {}\".format(len(train_loader)))\n",
    "for idx, data in enumerate(train_loader):\n",
    "    if idx % 1000 == 0:\n",
    "        print(\"Batch: {}\".format(idx))\n",
    "    q_len = len(data[0])\n",
    "    p_len = len(data[1])\n",
    "    d_len = len(data[2])\n",
    "    if p_len > d_len:\n",
    "        seq_len = config.get_config(section=\"ml\", key=\"pep_seq_len\")  # + charge\n",
    "        zero_pad = torch.zeros(p_len - d_len, seq_len, dtype=torch.long)\n",
    "        data[2] = torch.cat((data[2], zero_pad))\n",
    "    data[0] = data[0].to(rank)  # spectra\n",
    "    data[1] = data[1].to(rank)  # peptides\n",
    "    data[2] = data[2].to(rank)  # decoys\n",
    "    Q, P, D = snap_model(data[:-1])\n",
    "    Q = Q.detach().cpu().numpy()\n",
    "    P = P.detach().cpu().numpy()\n",
    "    D = D.detach().cpu().numpy()\n",
    "    q.append(Q)\n",
    "    p.append(P)\n",
    "    d.append(D)\n",
    "    l_spec_labels = np.repeat(np.arange(data[-1].size), data[-1])\n",
    "    train_spec_labels.append(l_spec_labels)\n",
    "    train_spec_charges.append(data[3])\n",
    "    train_spec_masses.append(data[4])\n",
    "\n",
    "q = np.concatenate(q)\n",
    "p = np.concatenate(p)\n",
    "d = np.concatenate(d)\n",
    "train_spec_labels = np.concatenate(train_spec_labels)\n",
    "train_spec_charges = np.concatenate(train_spec_charges)\n",
    "train_spec_masses = np.concatenate(train_spec_masses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort q, train_dataset.np_specs, train_spec_charges by train_spec_masses\n",
    "print(\"Sorting data by mass...\")\n",
    "zipped = zip(q, np_specs, train_spec_charges, train_spec_masses)\n",
    "print(len(q), len(np_specs), len(train_spec_charges), len(train_spec_masses))\n",
    "sorted_zipped = sorted(zipped, key=lambda x: x[-1])\n",
    "q, np_specs, train_spec_charges, train_spec_masses = zip(*sorted_zipped)\n",
    "\n",
    "print(\"Saving training data embeddings...\")\n",
    "np.save(\"training_data/q.npy\", q)\n",
    "np.save(\"training_data/p.npy\", p)\n",
    "np.save(\"training_data/d.npy\", d)\n",
    "np.save(\"training_data/spec_labels.npy\", train_spec_labels)\n",
    "np.save(\"training_data/charges.npy\", train_spec_charges)\n",
    "np.save(\"training_data/masses.npy\", train_spec_masses)\n",
    "pickle.dump(np_specs, open(\"training_data/np_specs.pkl\", \"wb\"))\n",
    "pickle.dump(train_peps_strings, open(\"training_data/peps.pkl\", \"wb\"))\n",
    "pickle.dump(train_dpeps_strings, open(\"training_data/dpeps.pkl\", \"wb\"))\n",
    "pickle.dump(train_peps_masses, open(\"training_data/pep_masses.pkl\", \"wb\"))\n",
    "pickle.dump(train_dpeps_masses, open(\"training_data/dpep_masses.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(p), len(train_peps_masses), len(train_masses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load q, p, d, spec_labels, charges, peps, dpeps\n",
    "q = np.load(\"training_data/q.npy\")\n",
    "p = np.load(\"training_data/p.npy\")\n",
    "d = np.load(\"training_data/d.npy\")\n",
    "# train_spec_labels = np.load(\"training_data/spec_labels.npy\")\n",
    "train_spec_charges = np.load(\"training_data/charges.npy\")\n",
    "train_spec_masses = np.load(\"training_data/masses.npy\").flatten()\n",
    "train_np_specs = pickle.load(open(\"training_data/np_specs.pkl\", \"rb\"))\n",
    "# train_peps_strings = pickle.load(open(\"training_data/peps.pkl\", \"rb\"))\n",
    "# train_dpeps_strings = pickle.load(open(\"training_data/dpeps.pkl\", \"rb\"))\n",
    "train_peps_masses = pickle.load(open(\"training_data/pep_masses.pkl\", \"rb\"))\n",
    "train_dpeps_masses = pickle.load(open(\"training_data/dpep_masses.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1498374\n"
     ]
    }
   ],
   "source": [
    "print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16582\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# batch_size  = config.get_config(section=\"ml\", key=\"batch_size\")\n",
    "in_tensor_dir_pt = \"/lclhome/mtari008/data/deepsnap/proteome-tools/hcd\"\n",
    "# in_tensor_dir = \"/disk/raptor-2/mtari008/data/deepsnap/train-ready/proteome-tools/\"\n",
    "\n",
    "with open(join(in_tensor_dir_pt, \"pep_spec.pkl\"), \"rb\") as f:\n",
    "    dir_listing = pickle.load(f)\n",
    "\n",
    "# random.shuffle(dir_listing)\n",
    "\n",
    "dir_listing = sorted(dir_listing, key=lambda x: float(re.search(r\"(\\d+)-(\\d+.\\d+).[pep]\", x[0])[2]))\n",
    "pre_mass = 2000\n",
    "pre_mass_max = 6000\n",
    "tol = 100\n",
    "len_spec_list = 10\n",
    "total_count = 10000000\n",
    "min_mass = pre_mass - 1  # ppm(pre_mass, tol)\n",
    "max_mass = pre_mass + 1  # ppm(pre_mass, tol)\n",
    "\n",
    "l_pep_file_names = []\n",
    "l_spec_file_names_lists = []\n",
    "num_specs = []\n",
    "spec_count = 0\n",
    "pep_set = set()\n",
    "for pep, spec_list in dir_listing:\n",
    "    pep_mass = float(re.search(r\"(\\d+)-(\\d+.\\d+).[pep]\", pep)[2])\n",
    "    #     if len(spec_list) >= 3 and min_mass < pep_mass < max_mass:\n",
    "    if len(spec_list) >= len_spec_list and pre_mass_max >= pep_mass >= pre_mass:\n",
    "        # print(pep_mass)\n",
    "        #         print(pep)\n",
    "        #         print(spec_list)\n",
    "        l_pep_file_names.append(pep)\n",
    "        l_spec_file_names_lists.append(spec_list)\n",
    "        num_specs.append(len(spec_list))\n",
    "        spec_count += 1\n",
    "        if spec_count >= total_count:\n",
    "            break\n",
    "\n",
    "assert len(l_pep_file_names) == len(l_spec_file_names_lists)\n",
    "print(len(l_pep_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_by_percentage(array, percentage):\n",
    "    if percentage < 0 or percentage > 1:\n",
    "        raise ValueError(\"Percentage should be between 0 and 1\")\n",
    "\n",
    "    if percentage == 0:\n",
    "        return array\n",
    "\n",
    "    num_columns_to_drop = int(array.shape[1] * percentage)\n",
    "    random_indices = random.sample(range(array.shape[1]), num_columns_to_drop)\n",
    "    column_mask = np.ones(array.shape[1], dtype=bool)\n",
    "    column_mask[random_indices] = False\n",
    "    return array[:, column_mask]\n",
    "\n",
    "\n",
    "def get_dense_tensor_from_sparse_numpy(sparse_array, means, stds, spec_size, drop_percentage=0.0):\n",
    "    sparse_array = drop_columns_by_percentage(sparse_array, drop_percentage)\n",
    "    ind = torch.LongTensor(np.array([[0] * sparse_array.shape[1], sparse_array[0]]))\n",
    "    val = torch.FloatTensor(sparse_array[1])\n",
    "    dense_array = torch.sparse_coo_tensor(ind, val, torch.Size([1, spec_size])).to_dense()\n",
    "    return (dense_array - means) / stds\n",
    "\n",
    "\n",
    "def pad_left(arr, size):\n",
    "    out = np.zeros(size)\n",
    "    out[-len(arr) :] = arr\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_pep(pep_file_name, seq_len):\n",
    "    with open(pep_file_name, \"r\") as f:\n",
    "        pep = f.readlines()[0].strip()\n",
    "\n",
    "    pepl = [config.aa2idx[aa] for aa in pep]\n",
    "    pepl = pad_left(pepl, seq_len)\n",
    "    torch_pep = torch.tensor(pepl, dtype=torch.long)\n",
    "\n",
    "    return torch_pep, pep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,)\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "seq_len = config.get_config(section=\"ml\", key=\"pep_seq_len\")\n",
    "pep_path = join(in_tensor_dir_pt, \"peptides\")\n",
    "train_peps_strings = []\n",
    "test_pep_strings = []\n",
    "\n",
    "spec_path = join(in_tensor_dir_pt, \"spectra\")\n",
    "spec_size = config.get_config(section=\"input\", key=\"spec_size\")\n",
    "\n",
    "means = np.load(join(in_tensor_dir_pt, \"means.npy\"))\n",
    "stds = np.load(join(in_tensor_dir_pt, \"stds.npy\"))\n",
    "means = torch.from_numpy(means).float()\n",
    "stds = torch.from_numpy(stds).float()\n",
    "\n",
    "torch_spec_list, test_np_specs, test_spec_charge_list, test_spec_mass_list = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "# allowed_charges = {2, 3, 4}\n",
    "allowed_charges = {2}\n",
    "# percentages = [0.0, 0.05, 0.1, 0.15]\n",
    "# percentages = [0.0]\n",
    "percentages = [0.0, 0.1, 0.2]\n",
    "num_specs = []\n",
    "keep = 2000\n",
    "pep_count = 0\n",
    "for spec_file_list, pep_file_name in zip(l_spec_file_names_lists, l_pep_file_names):\n",
    "    (\n",
    "        l_test_np_specs,\n",
    "        l_torch_spec_list,\n",
    "        l_test_spec_charge_list,\n",
    "        l_test_spec_mass_list,\n",
    "    ) = ([], [], [], [])\n",
    "    pep_mass = float(re.search(r\"(\\d+)-(\\d+.\\d+).[pep]\", pep_file_name)[2])\n",
    "    pep_path = join(in_tensor_dir_pt, \"peptides\")\n",
    "    pep_file_name = join(pep_path, pep_file_name)\n",
    "    torch_pep, pep = get_pep(pep_file_name, seq_len)\n",
    "    found_charges = set()\n",
    "    spec_count = 0\n",
    "    for spec_file in spec_file_list:\n",
    "        file_parts = re.search(r\"(\\d+)\\.(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d+)-(\\d+).[pt|npy]\", spec_file)\n",
    "        spec_mass = float(file_parts[4])\n",
    "        spec_charge = int(file_parts[5])\n",
    "        # if spec_charge not in allowed_charges or spec_charge in found_charges: # Uncomment to enable for variation metric\n",
    "        #     continue\n",
    "        found_charges.add(spec_charge)\n",
    "        np_spec = np.load(join(spec_path, spec_file))\n",
    "        for drop_percentage in percentages:\n",
    "            spec_count += 1\n",
    "            torch_spec = get_dense_tensor_from_sparse_numpy(\n",
    "                np_spec, means, stds, spec_size, drop_percentage=drop_percentage\n",
    "            )\n",
    "            l_test_np_specs.append(np_spec)\n",
    "            l_torch_spec_list.append(torch_spec)\n",
    "            l_test_spec_charge_list.append(spec_charge)\n",
    "            l_test_spec_mass_list.append(pep_mass)\n",
    "\n",
    "        break  # remove to enable for variation metric\n",
    "\n",
    "    if spec_count == len(percentages) * len(allowed_charges):  # Uncomment to enable for variation metric\n",
    "        # if spec_count > 0: # Uncomment for other metrics\n",
    "        num_specs.append(spec_count)\n",
    "        train_peps_strings.append(torch_pep)\n",
    "        test_pep_strings.append(pep)\n",
    "        (\n",
    "            l_test_np_specs,\n",
    "            l_torch_spec_list,\n",
    "            l_test_spec_charge_list,\n",
    "            l_test_spec_mass_list,\n",
    "        ) = zip(\n",
    "            *sorted(\n",
    "                zip(\n",
    "                    l_test_np_specs,\n",
    "                    l_torch_spec_list,\n",
    "                    l_test_spec_charge_list,\n",
    "                    l_test_spec_mass_list,\n",
    "                ),\n",
    "                key=lambda x: x[2],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        test_np_specs.extend(l_test_np_specs)\n",
    "        torch_spec_list.extend(l_torch_spec_list)\n",
    "        test_spec_charge_list.extend(l_test_spec_charge_list)\n",
    "        test_spec_mass_list.extend(l_test_spec_mass_list)\n",
    "        pep_count += 1\n",
    "\n",
    "    if pep_count >= keep:\n",
    "        break\n",
    "\n",
    "    # else:\n",
    "    #     # pop the count values from torch_spec_list, spec_charge_list, spec_mass_list\n",
    "    #     for i in range(count):\n",
    "    #         torch_spec_list.pop()\n",
    "    #         test_np_specs.pop()\n",
    "    #         test_spec_charge_list.pop()\n",
    "    #         test_spec_mass_list.pop()\n",
    "\n",
    "    # torch_spec_charge_list = [self.charge2idx[charge] for charge in torch_spec_charge_list]\n",
    "test_spec_charges = np.array(test_spec_charge_list)\n",
    "test_spec_masses = np.array(test_spec_mass_list)\n",
    "print(test_spec_charges.shape)\n",
    "torch_specs = torch.cat(torch_spec_list, dim=0)\n",
    "torch_peps = torch.stack(train_peps_strings, dim=0)\n",
    "print(len(torch_peps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_collate(batch):\n",
    "    # specs = torch.stack([item for item in batch], 0)\n",
    "    dummy_pep = np.zeros(config.get_config(section=\"ml\", key=\"pep_seq_len\"))\n",
    "    dummy_pep = torch.from_numpy(dummy_pep).long().unsqueeze(0)\n",
    "    return [batch, dummy_pep]\n",
    "\n",
    "\n",
    "def pep_collate(batch):\n",
    "    # peps = torch.stack([item for item in batch], 0)\n",
    "    dummy_spec = np.zeros(config.get_config(section=\"input\", key=\"spec_size\"))\n",
    "    dummy_spec = torch.from_numpy(dummy_spec).float().unsqueeze(0)\n",
    "    dummy_pep = np.zeros((2, config.get_config(section=\"ml\", key=\"pep_seq_len\")))\n",
    "    dummy_pep = torch.from_numpy(dummy_pep).long()  # .unsqueeze(0)\n",
    "    # tqdm.write(\"{}\".format(peps.shape))\n",
    "    # tqdm.write(\"{}\".format(dummy_pep.shape))\n",
    "    return [dummy_spec, batch, dummy_pep]\n",
    "\n",
    "\n",
    "def runModel(batch, s_model, in_type, device):\n",
    "    with torch.no_grad():\n",
    "        accurate_labels = 0\n",
    "        all_labels = 0\n",
    "        loss = 0\n",
    "        out_out = torch.Tensor().cpu()\n",
    "        batch[0], batch[1] = batch[0].to(device), batch[1].to(device)\n",
    "        if in_type == \"specs\":\n",
    "            out_ = s_model(batch, data_type=in_type)[0]\n",
    "        elif in_type == \"peps\":\n",
    "            batch[2] = batch[2].to(device)\n",
    "            out_ = s_model(batch, data_type=in_type)[0]\n",
    "        out_out = torch.cat((out_out, out_.to(\"cpu\")), dim=0)\n",
    "        # bar.update(batch_idx)\n",
    "    return out_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "torch_specs_collated = spec_collate(torch_specs)\n",
    "torch_peps_collated = pep_collate(torch_peps)\n",
    "test_e_specs = runModel(torch_specs_collated, snap_model, \"specs\", rank).detach().numpy()\n",
    "test_e_peps = runModel(torch_peps_collated, snap_model, \"peps\", rank).detach().numpy()\n",
    "test_spec_charges = np.array(test_spec_charge_list).astype(int)\n",
    "num_specs = np.array(num_specs)\n",
    "test_spec_labels = np.repeat(np.arange(num_specs.size), num_specs)\n",
    "\n",
    "print(len(torch_peps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_append = '_rmv_peaks_augmnts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_e_specs, test_np_specs, test_spec_charges, test_spec_masses = zip(\n",
    "#     *sorted(zip(test_e_specs, test_np_specs, test_spec_charges, test_spec_masses), key=lambda x: x[-1])\n",
    "# )\n",
    "np.save(f\"proteome_tools_data/hcd/e_specs{save_append}.npy\", test_e_specs)\n",
    "pickle.dump(test_np_specs, open(f\"proteome_tools_data/hcd/np_specs{save_append}.pkl\", \"wb\"))\n",
    "np.save(f\"proteome_tools_data/hcd/e_peps{save_append}.npy\", test_e_peps)\n",
    "np.save(f\"proteome_tools_data/hcd/spec_charges{save_append}.npy\", test_spec_charges)\n",
    "np.save(f\"proteome_tools_data/hcd/spec_masses{save_append}.npy\", test_spec_masses)\n",
    "np.save(f\"proteome_tools_data/hcd/spec_labels{save_append}.npy\", test_spec_labels)\n",
    "pickle.dump(test_pep_strings, open(f\"proteome_tools_data/hcd/peps{save_append}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_e_specs = np.load(f\"proteome_tools_data/hcd/e_specs{save_append}.npy\")\n",
    "test_np_specs = pickle.load(open(f\"proteome_tools_data/hcd/np_specs{save_append}.pkl\", \"rb\"))\n",
    "test_e_peps = np.load(f\"proteome_tools_data/hcd/e_peps{save_append}.npy\")\n",
    "test_spec_charges = np.load(f\"proteome_tools_data/hcd/spec_charges{save_append}.npy\")\n",
    "test_spec_masses = np.load(f\"proteome_tools_data/hcd/spec_masses{save_append}.npy\")\n",
    "# test_spec_labels = np.load(\"proteome_tools_data/spec_labels.npy\")\n",
    "# test_pep_strings = pickle.load(open(\"proteome_tools_data/peps.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_e_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(values):\n",
    "    min_val, max_val = min(values), max(values)\n",
    "    vals = np.array(values)\n",
    "    normalized_values = (vals - min_val) / (max_val - min_val)\n",
    "    print(min_val, max_val)\n",
    "    print(normalized_values)\n",
    "    return normalized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.special import kl_div\n",
    "from scipy.spatial.distance import jaccard\n",
    "\n",
    "\n",
    "def compute_similarity_metrics(arr1, arr2):\n",
    "    # Checking if both arrays have the same length\n",
    "    if len(arr1) != len(arr2):\n",
    "        raise ValueError(\"Both arrays must have the same length\")\n",
    "\n",
    "    # Reshaping to row vectors for compatibility with cosine_similarity\n",
    "    arr1_2d, arr2_2d = np.array(arr1).reshape(1, -1), np.array(arr2).reshape(1, -1)\n",
    "\n",
    "    # Flattening arrays for compatibility with pearsonr and spearmanr\n",
    "    arr1_1d, arr2_1d = arr1_2d.flatten(), arr2_2d.flatten()\n",
    "\n",
    "    # Pearson Correlation\n",
    "    pearson_corr, _ = stats.pearsonr(arr1_1d, arr2_1d)\n",
    "\n",
    "    # Spearman Correlation\n",
    "    spearman_corr, _ = stats.spearmanr(arr1_1d, arr2_1d)\n",
    "\n",
    "    # Mean Squared Error\n",
    "    mse = mean_squared_error(arr1_1d, arr2_1d)\n",
    "\n",
    "    # Root Mean Squared Error\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = cosine_similarity(arr1_2d, arr2_2d)[0][0]\n",
    "\n",
    "    # KL Divergence\n",
    "    kl_divergence = kl_div(arr1_1d, arr2_1d).sum()\n",
    "\n",
    "    # Jaccard Similarity\n",
    "    jaccard_sim = 1 - jaccard(arr1_1d, arr2_1d)\n",
    "\n",
    "    return {\n",
    "        \"pearson_corr\": pearson_corr,\n",
    "        \"spearman_corr\": spearman_corr,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"cosine_sim\": cosine_sim,\n",
    "        \"kl_divergence\": kl_divergence,\n",
    "        \"jaccard_sim\": jaccard_sim,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# k = len(percentages) * len(allowed_charges)\n",
    "k = 1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_score(e_specs, e_peps, k):\n",
    "    # Check if len(e_specs) = k * len(e_peps)\n",
    "    if len(e_specs) != k * len(e_peps):\n",
    "        raise ValueError(\"len(e_specs) must be equal to k * len(e_peps)\")\n",
    "\n",
    "    # Reshape e_peps to match the shape of e_specs for broadcasting\n",
    "    e_peps_rep = np.repeat(e_peps, k, axis=0)\n",
    "\n",
    "    # Calculate squared L2 distance between e_specs and e_peps\n",
    "    sq_dist = np.sum((e_specs - e_peps_rep) ** 2, axis=1)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    sq_dist = np.where(sq_dist == 0, 1e-10, sq_dist)\n",
    "\n",
    "    # Calculate SNAP score\n",
    "    snap_scores = 1 / sq_dist\n",
    "\n",
    "    return snap_scores\n",
    "\n",
    "\n",
    "import bisect\n",
    "\n",
    "from torch import le\n",
    "\n",
    "\n",
    "def snap_score_2(e_specs, e_peps, spec_masses, pep_masses, mass_tol, k):\n",
    "    # Check if len(e_specs) = k * len(e_peps)\n",
    "    if len(e_specs) != k * len(e_peps):\n",
    "        raise ValueError(\"len(e_specs) must be equal to k * len(e_peps)\")\n",
    "\n",
    "    # Reshape e_peps to match the shape of e_specs for broadcasting\n",
    "    e_peps_rep = np.repeat(e_peps, k, axis=0)\n",
    "\n",
    "    # Calculate squared L2 distance between e_specs and e_peps\n",
    "    sq_dist = np.sum((e_specs - e_peps_rep) ** 2, axis=1)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    sq_dist = np.where(sq_dist == 0, 1e-10, sq_dist)\n",
    "\n",
    "    # Calculate SNAP score\n",
    "    snap_scores = 1 / sq_dist\n",
    "\n",
    "    # Sorting pep_masses and e_peps according to pep_masses\n",
    "    sort_indices = np.argsort(pep_masses)\n",
    "    pep_masses_sorted = pep_masses[sort_indices]\n",
    "    e_peps_sorted = e_peps[sort_indices]\n",
    "\n",
    "    snap_scores_max = []  # To store the highest snap scores within mass_tol range\n",
    "    max_score_pep_ids = []  # To store the pep_ids corresponding to the highest snap scores\n",
    "\n",
    "    for i in range(len(spec_masses)):\n",
    "        # Binary search to find the range of peptides within mass_tol range\n",
    "        lower_index = bisect.bisect_left(pep_masses_sorted, spec_masses[i] - mass_tol)\n",
    "        upper_index = bisect.bisect_right(pep_masses_sorted, spec_masses[i] + mass_tol)\n",
    "\n",
    "        # Compute squared L2 distance for peptides within mass_tol range\n",
    "        sq_dist = np.sum((e_specs[i] - e_peps_sorted[lower_index:upper_index]) ** 2, axis=1)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        sq_dist = np.where(sq_dist == 0, 1e-10, sq_dist)\n",
    "\n",
    "        # Calculate SNAP scores for peptides within mass_tol range\n",
    "        snap_scores_range = 1 / sq_dist\n",
    "\n",
    "        # Find the highest SNAP score\n",
    "        max_index = np.argmax(snap_scores_range)\n",
    "        max_score_pep_ids.append(lower_index + max_index)\n",
    "        snap_scores_max.append(snap_scores_range[max_index])\n",
    "\n",
    "    # True if max_score_pep_ids values are the same as index\n",
    "    is_correct = np.array(max_score_pep_ids) == np.repeat(np.arange(len(e_peps)), k)  # Labels\n",
    "\n",
    "    return snap_scores, snap_scores_max, is_correct\n",
    "\n",
    "\n",
    "from bisect import bisect_left, bisect_right\n",
    "\n",
    "\n",
    "def snap_score_3(e_specs, e_peps, spec_masses, pep_masses, mass_tol):\n",
    "    # Check if len(e_specs) and len(spec_masses) are the same, similarly for e_peps and pep_masses\n",
    "    if len(e_specs) != len(spec_masses) or len(e_peps) != len(pep_masses):\n",
    "        raise ValueError(\"Input arrays should have the same length\")\n",
    "\n",
    "    # Initialize an array to hold the snap scores\n",
    "    snap_scores_max = np.zeros(len(e_specs))\n",
    "    print(len(e_specs))\n",
    "    print(len(e_peps))\n",
    "\n",
    "    unmatched = 0\n",
    "\n",
    "    # Compute the snap scores for each spectrum\n",
    "    for i, (e_spec, spec_mass) in enumerate(zip(e_specs, spec_masses)):\n",
    "        # Determine the lower and upper bounds for the mass tolerance\n",
    "        lower_bound = spec_mass - mass_tol\n",
    "        upper_bound = spec_mass + mass_tol\n",
    "\n",
    "        # Find the indices of peptides within the mass tolerance range\n",
    "        start_index = bisect_left(pep_masses, lower_bound)\n",
    "        end_index = bisect_right(pep_masses, upper_bound)\n",
    "\n",
    "        # Get the peptides within the mass tolerance range\n",
    "        e_peps_in_range = e_peps[start_index:end_index]\n",
    "\n",
    "        # Calculate squared L2 distances\n",
    "        sq_dists = np.sum((e_spec - e_peps_in_range) ** 2, axis=1)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        sq_dists = np.where(sq_dists == 0, 1e-10, sq_dists)\n",
    "        if len(sq_dists) == 0:\n",
    "            # print('empty')\n",
    "            # print(i)\n",
    "            # print(spec_mass)\n",
    "            # print(lower_bound)\n",
    "            # print(len(e_peps_in_range))\n",
    "            # print(start_index)\n",
    "            # print(end_index)\n",
    "            # print(pep_masses[0])\n",
    "            # print(pep_masses[-1])\n",
    "            unmatched += 1\n",
    "            snap_scores_max[i] = 0\n",
    "        else:\n",
    "            # Calculate and store the maximum SNAP score\n",
    "            snap_scores_max[i] = np.max(1.0 / sq_dists)\n",
    "\n",
    "    print(\"Unmatched: {} / {}\".format(unmatched, len(e_specs)))\n",
    "\n",
    "    return snap_scores_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1498374\n",
      "Unmatched: 0 / 6000\n"
     ]
    }
   ],
   "source": [
    "test_snap_scores = snap_score(test_e_specs[0::k], test_e_peps, k=3)\n",
    "\n",
    "test_snap_scores_3 = snap_score_3(test_e_specs[0::k], p, test_spec_masses[0::k], train_peps_masses, 100)\n",
    "\n",
    "test_snap_scores_2, test_snap_scores_max, test_is_correct = snap_score_2(\n",
    "    test_e_specs[::k], test_e_peps, test_spec_masses[::k], test_spec_masses[::3], 1, k=3\n",
    ")\n",
    "# test_snap_scores = normalize_values(test_snap_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9381666666666667\n",
      "5629\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_is_correct) / len(test_is_correct))\n",
    "print(sum(test_is_correct))\n",
    "print(len(test_is_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_variances(specs, k, keepdims=False):\n",
    "    # Check if the number of vectors is divisible by k\n",
    "    if len(specs) % k != 0:\n",
    "        raise ValueError(\"The number of vectors must be divisible by k\")\n",
    "\n",
    "    # Calculate the number of chunks\n",
    "    num_chunks = len(specs) // k\n",
    "\n",
    "    # Create a new array of the right size, with each element initialized to zero\n",
    "    sum_of_variances = np.zeros(num_chunks)\n",
    "\n",
    "    # Use numpy's advanced indexing to create an array of shape (num_chunks, k, m)\n",
    "    chunks = specs.reshape(num_chunks, k, -1)\n",
    "\n",
    "    # Calculate the variance along the second axis (the k vectors)\n",
    "    # then sum along the last axis (the m dimensions)\n",
    "    sum_of_variances = (\n",
    "        np.var(chunks, axis=1).sum(axis=-1) if not keepdims else np.var(chunks, axis=1).sum(axis=-1).repeat(k)\n",
    "    )\n",
    "\n",
    "    return sum_of_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "test_e_specs = np.array(test_e_specs)\n",
    "traces = sum_of_variances(test_e_specs, k=3, keepdims=True)\n",
    "print(traces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_traces = normalize_values(traces)\n",
    "print(max(normalized_traces), min(normalized_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(\n",
    "    data,\n",
    "    baseline_value=0.64,\n",
    "    baseline_color=\"red\",\n",
    "    bins=None,\n",
    "    title=\"Distribution of Floating Point Numbers\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Frequency\",\n",
    "):\n",
    "    # Plot the histogram\n",
    "    n, bin_edges, patches = plt.hist(data, bins=bins, alpha=0.75)\n",
    "\n",
    "    # Set the baseline height to the maximum height of the histogram\n",
    "    baseline_height = max(n)\n",
    "\n",
    "    # Add the baseline bar\n",
    "    plt.bar(\n",
    "        baseline_value,\n",
    "        baseline_height,\n",
    "        width=np.diff(bin_edges)[0],\n",
    "        color=baseline_color,\n",
    "        align=\"center\",\n",
    "    )\n",
    "\n",
    "    # Set the x-axis limits to include the baseline value\n",
    "    # plt.xlim(min(data + [baseline_value]), max(data + [baseline_value + 0.1]))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(\n",
    "    traces,\n",
    "    bins=100,\n",
    "    title=\"Distribution of Per-Sample Feature Variations\",\n",
    "    xlabel=\"Per-Sample Feature Variations\",\n",
    "    ylabel=\"Frequency\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(normalize_values(test_snap_scores), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_vs_trace = compute_similarity_metrics(normalize_values(test_snap_scores), normalize_values(1 / traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snap_vs_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gmm_and_estimate_density(data, n_components=3):\n",
    "    # Fit a Gaussian Mixture Model to the data\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    gmm.fit(data)\n",
    "\n",
    "    # Estimate the density around each vector in the data\n",
    "    log_prob_density = gmm.score_samples(data)\n",
    "\n",
    "    return log_prob_density\n",
    "\n",
    "\n",
    "prob_density = fit_gmm_and_estimate_density(e_peps, n_components=20)\n",
    "print(len(prob_density), len(e_peps))\n",
    "print(prob_density)\n",
    "print(max(prob_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vmf_and_estimate_density(data, n_clusters=3, max_iter=100, tol=1e-3):\n",
    "    # Fit a von Mises-Fisher Mixture Model to the data\n",
    "    vmf = VonMisesFisherMixture(n_clusters=n_clusters, max_iter=max_iter, tol=tol)\n",
    "    vmf.fit(data)\n",
    "\n",
    "    # Estimate the density around each vector in the data\n",
    "    # density_estimates = vmf.predict_proba(data)\n",
    "    # density_estimates = vmf.score(data)\n",
    "    density_estimates = vmf.log_likelihood(data)\n",
    "\n",
    "    return density_estimates\n",
    "\n",
    "\n",
    "def fit_vmf_and_estimate_density_1(data, n_clusters=3, max_iter=100, tol=1e-3):\n",
    "    m = data.shape[1]\n",
    "\n",
    "    # Fit a von Mises-Fisher Mixture Model to the data\n",
    "    vmf = VonMisesFisherMixture(n_clusters=n_clusters, max_iter=max_iter, tol=tol)\n",
    "    vmf.fit(data)\n",
    "\n",
    "    # Compute the log of the density estimate for each data point\n",
    "    log_exp_terms = np.dot(data, vmf.cluster_centers_.T) * vmf.concentrations_\n",
    "    log_const_terms = (np.log(vmf.concentrations_) * ((m - 1) / 2)) - (\n",
    "        np.log(2 * np.pi) * (m / 2) + np.log(iv((m - 1) / 2, vmf.concentrations_))\n",
    "    )\n",
    "    log_density_estimate = logsumexp(log_const_terms + log_exp_terms, axis=1, b=vmf.weights_)\n",
    "\n",
    "    # Normalize the log density estimates and convert back to the density estimate\n",
    "    max_log_density_estimate = np.max(log_density_estimate)\n",
    "    shifted_log_density_estimate = log_density_estimate - max_log_density_estimate\n",
    "    sum_exp_shifted_log_density_estimate = np.sum(np.exp(shifted_log_density_estimate))\n",
    "    normalized_log_density_estimate = shifted_log_density_estimate - np.log(sum_exp_shifted_log_density_estimate)\n",
    "    density_estimate = np.exp(normalized_log_density_estimate)\n",
    "\n",
    "    return log_density_estimate\n",
    "\n",
    "\n",
    "def fit_vmf_and_estimate_density_2(data, p, n_clusters=3, max_iter=100, tol=1e-3):\n",
    "    m = p.shape[1]\n",
    "\n",
    "    # Fit a von Mises-Fisher Mixture Model to the training data p\n",
    "    vmf = VonMisesFisherMixture(n_clusters=n_clusters, max_iter=max_iter, tol=tol)\n",
    "    vmf.fit(\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    # Compute the log of the density estimate for each data point\n",
    "    log_exp_terms = np.dot(data, vmf.cluster_centers_.T) * vmf.concentrations_\n",
    "    log_const_terms = (np.log(vmf.concentrations_) * ((m - 1) / 2)) - (\n",
    "        np.log(2 * np.pi) * (m / 2) + np.log(iv((m - 1) / 2, vmf.concentrations_))\n",
    "    )\n",
    "    log_density_estimate = logsumexp(log_const_terms + log_exp_terms, axis=1, b=vmf.weights_)\n",
    "\n",
    "    # Normalize the log density estimates and convert back to the density estimate\n",
    "    max_log_density_estimate = np.max(log_density_estimate)\n",
    "    shifted_log_density_estimate = log_density_estimate - max_log_density_estimate\n",
    "    sum_exp_shifted_log_density_estimate = np.sum(np.exp(shifted_log_density_estimate))\n",
    "    normalized_log_density_estimate = shifted_log_density_estimate - np.log(sum_exp_shifted_log_density_estimate)\n",
    "    density_estimate = np.exp(normalized_log_density_estimate)\n",
    "\n",
    "    return log_density_estimate\n",
    "\n",
    "\n",
    "def normalize_values(values):\n",
    "    min_val, max_val = min(values), max(values)\n",
    "    vals = np.array(values)\n",
    "    normalized_values = (vals - min_val) / (max_val - min_val)\n",
    "    print(min_val, max_val)\n",
    "    print(normalized_values)\n",
    "    return normalized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_estimates(\n",
    "    density_estimates,\n",
    "    title=\"Density Estimates\",\n",
    "    xlabel=\"Vector Index\",\n",
    "    ylabel=\"Density\",\n",
    "):\n",
    "    n_vectors = len(density_estimates)\n",
    "    vector_indices = np.arange(n_vectors)\n",
    "\n",
    "    # Create a bar plot of the density estimates\n",
    "    plt.bar(vector_indices, density_estimates)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(vector_indices)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_distribution(\n",
    "    density_estimates,\n",
    "    bins=None,\n",
    "    title=\"Density Distribution\",\n",
    "    xlabel=\"Density\",\n",
    "    ylabel=\"Frequency\",\n",
    "):\n",
    "    # Create a histogram of the density estimates\n",
    "    plt.hist(density_estimates, bins=bins, alpha=0.75)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.xlim(min(density_estimates), max(density_estimates))\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_estimates_peps = fit_vmf_and_estimate_density_1(test_e_peps, n_clusters=20)\n",
    "print(density_estimates_peps)\n",
    "print(min(density_estimates_peps), max(density_estimates_peps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_estimates_peps = normalize_values(density_estimates_peps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_estimates(density_estimates_peps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_distribution(density_estimates_peps, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_density_specs = fit_vmf_and_estimate_density_2(test_e_specs[::k], p, n_clusters=3)\n",
    "prob_density_specs = np.load(\"outputs/prob_density_specs.npy\")[0::k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = normalize_values(prob_density_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_estimates(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_distribution(density, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_vs_density = compute_similarity_metrics(test_snap_scores, prob_density_specs)\n",
    "print(snap_vs_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming ref_scores, score1, score2, score3 are your score arrays\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    normalize_values(test_snap_scores_3),\n",
    "    normalize_values(prob_density_specs),\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.xlabel(\"SNAP Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"SNAP Score vs Density\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(normalize_values(test_snap_scores), normalize_values(1 - density), c=test_spec_charges[::k], alpha=0.5)\n",
    "# plt.xlabel('SNAP Score')\n",
    "# plt.ylabel('1 - Density')\n",
    "# plt.title('SNAP Score vs Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bisect import bisect_left, bisect_right\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def find_indices_within_tolerance(precursor_masses, target_mass, tol):\n",
    "    lower_mass = target_mass - tol\n",
    "    upper_mass = target_mass + tol\n",
    "\n",
    "    lower_index = bisect_left(precursor_masses, lower_mass)\n",
    "    upper_index = bisect_right(precursor_masses, upper_mass)\n",
    "\n",
    "    return lower_index, upper_index\n",
    "\n",
    "\n",
    "def spectral_similarity(spec1, spec2):\n",
    "    mz1 = spec1[0].astype(int)\n",
    "    mz2 = spec2[0].astype(int)\n",
    "\n",
    "    intensity1 = spec1[1]\n",
    "    intensity2 = spec2[1]\n",
    "\n",
    "    dot_product = 0\n",
    "    i, j = 0, 0\n",
    "    while i < len(mz1) and j < len(mz2):\n",
    "        if mz1[i] == mz2[j]:\n",
    "            dot_product += intensity1[i] * intensity2[j]\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif mz1[i] < mz2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    norm1 = np.linalg.norm(intensity1)\n",
    "    norm2 = np.linalg.norm(intensity2)\n",
    "\n",
    "    if norm1 != 0 and norm2 != 0:\n",
    "        similarity = dot_product / (norm1 * norm2)\n",
    "    else:\n",
    "        similarity = 0\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def msms_similarity(specs1, specs2, emb1, emb2, precursors1, precursors2, tol):\n",
    "    # Initialize results matrix as a sparse matrix\n",
    "    sim_matrix = lil_matrix((len(specs1), len(specs2)), dtype=np.float32)\n",
    "    emb_sim_matrix = lil_matrix((len(emb1), len(emb2)), dtype=np.float32)\n",
    "\n",
    "    # Iterate over all spectra in specs1\n",
    "    for i in tqdm(range(len(specs1)), desc=\"Calculating spectral similarity\"):\n",
    "        # Find spectra in specs2 within tolerance\n",
    "        start_index = bisect_left(precursors2, precursors1[i] - tol)\n",
    "        end_index = bisect_right(precursors2, precursors1[i] + tol)\n",
    "\n",
    "        for j in range(start_index, end_index):\n",
    "            # Calculate spectral similarity\n",
    "            sim_matrix[i, j] = spectral_similarity(specs1[i], specs2[j])\n",
    "\n",
    "            # Calculate embedding similarity\n",
    "            emb_sim_matrix[i, j] = cosine_similarity(emb1[i].reshape(1, -1), emb2[j].reshape(1, -1))[0][0]\n",
    "\n",
    "    return sim_matrix.tocsr(), emb_sim_matrix.tocsr()\n",
    "\n",
    "\n",
    "def normalize(values):\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    return (values - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def embedding_confidence(test_X, train_X, test_Y, train_Y, test_precursors, train_precursors, tol, k):\n",
    "    num_test = len(test_X)\n",
    "    consistency = np.zeros(num_test)\n",
    "    sim_X = np.zeros(num_test)\n",
    "    sim_Y = np.zeros(num_test)\n",
    "\n",
    "    # Get the spectral and embedding similarity matrices\n",
    "    spec_sim_matrix, emb_sim_matrix = msms_similarity(\n",
    "        test_X, train_X, test_Y, train_Y, test_precursors, train_precursors, tol\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(num_test), desc=\"Get top k and calculate confidence\"):\n",
    "        # Get the spectral and embedding similarities for the i-th test example\n",
    "        spec_similarities = spec_sim_matrix[i].toarray().flatten()\n",
    "        emb_similarities = emb_sim_matrix[i].toarray().flatten()\n",
    "\n",
    "        # Get the indices of the top-k most similar examples\n",
    "        top_k_spec_indices = np.argpartition(spec_similarities, -k)[-k:]\n",
    "        top_k_emb_indices = np.argpartition(emb_similarities, -k)[-k:]\n",
    "\n",
    "        # Calculate the mean similarities in the input and embedding spaces\n",
    "        mean_sim_X = np.mean(spec_similarities[top_k_spec_indices])\n",
    "        mean_sim_Y = np.mean(emb_similarities[top_k_emb_indices])\n",
    "\n",
    "        # Calculate the confidence score for the i-th test example\n",
    "        sim_X[i] = mean_sim_X\n",
    "        sim_Y[i] = mean_sim_Y\n",
    "        consistency[i] = np.abs(mean_sim_X - mean_sim_Y)\n",
    "\n",
    "    return consistency, sim_X, sim_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence = embedding_confidence(\n",
    "#     test_np_specs[::k], train_np_specs, test_e_specs[::k], q, test_spec_masses[::k], train_spec_masses, tol=10, k=5\n",
    "# )\n",
    "\n",
    "consistency = np.load(\"outputs/consistency.npy\")[0::k]\n",
    "sim_X = np.load(\"outputs/sim_X.npy\")[0::k]\n",
    "sim_Y = np.load(\"outputs/sim_Y.npy\")[0::k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_np_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sim_X, sim_Y = confidence\n",
    "print(len(data))\n",
    "print(min(data), max(data))\n",
    "data_sorts = sorted(data, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_X = sim_X[::k]\n",
    "data = data[::k]\n",
    "print(len(sim_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_estimates(sim_Y)  # Not density estimates, but confidence estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density_distribution(1 - sim_Y, bins=30)  # Not density distribution, but confidence distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_snap_scores = np.array(test_snap_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_snap_scores = test_snap_scores[sim_X != 0]\n",
    "sim_Y = sim_Y[sim_X != 0]\n",
    "data = data[sim_X != 0]\n",
    "sim_X = sim_X[sim_X != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_similarity_metrics(test_snap_scores_3, consistency))\n",
    "print(compute_similarity_metrics(test_snap_scores_3, sim_X))\n",
    "print(compute_similarity_metrics(test_snap_scores_3, sim_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6000\n",
      "spec_mass: 6000\n",
      "spec_charge: 6000\n",
      "variations: 6000\n",
      "density: 3060\n",
      "consistency: 3060\n",
      "sim_X: 3060\n",
      "sim_Y: 3060\n",
      "snap_1: 6000\n",
      "snap_2: 6000\n",
      "snap_label: 6000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb Cell 61\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data[key])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m df_all \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data\u001b[39m=\u001b[39;49mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdragon4/lclhome/mtari008/DeepSNAP/uncertainty_analysis/uncertainty-analysis-specs.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m df_all\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moutputs/df_all\u001b[39m\u001b[39m{\u001b[39;00msave_append\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepsnap-sk-0.22/lib/python3.10/site-packages/pandas/core/frame.py:662\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    656\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    657\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    660\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    661\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 662\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    663\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    664\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepsnap-sk-0.22/lib/python3.10/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepsnap-sk-0.22/lib/python3.10/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepsnap-sk-0.22/lib/python3.10/site-packages/pandas/core/internals/construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    665\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    668\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    669\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    671\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"id\": np.arange(len(test_snap_scores_3)),\n",
    "    \"spec_mass\": test_spec_masses[0::k],\n",
    "    \"spec_charge\": test_spec_charges[0::k],\n",
    "    \"variations\": traces,\n",
    "    \"density\": prob_density_specs,\n",
    "    \"consistency\": consistency,\n",
    "    \"sim_X\": sim_X,\n",
    "    \"sim_Y\": sim_Y,\n",
    "    \"snap_1\": test_snap_scores,\n",
    "    \"snap_2\": test_snap_scores_3,\n",
    "    \"snap_label\": test_is_correct,\n",
    "}\n",
    "\n",
    "# print lengths of each array\n",
    "for key in data.keys():\n",
    "    print(f\"{key}: {len(data[key])}\")\n",
    "\n",
    "df_all = pd.DataFrame(data=data)\n",
    "df_all.to_csv(f\"outputs/df_all{save_append}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(consistency, bins=30, edgecolor=\"black\")\n",
    "plt.title(\"Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(data, vert=False)\n",
    "plt.title(\"Boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, bins=30, cumulative=True, density=True, histtype=\"step\", edgecolor=\"black\")\n",
    "plt.title(\"CDF Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming score1 and score2 are your vectors\n",
    "traces = sim_X\n",
    "score2 = sim_Y\n",
    "\n",
    "# Calculate the means\n",
    "mean1 = np.mean(traces)\n",
    "mean2 = np.mean(score2)\n",
    "\n",
    "# Scatter Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(traces, score2, alpha=0.5)\n",
    "plt.axvline(\n",
    "    mean1,\n",
    "    color=\"r\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean Spectral Similarity: {mean1:.2f}\",\n",
    ")\n",
    "plt.axhline(\n",
    "    mean2,\n",
    "    color=\"b\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean Embedding Similarity: {mean2:.2f}\",\n",
    ")\n",
    "plt.title(\"Scatter Plot\")\n",
    "plt.xlabel(\"Spectral Similarity\")\n",
    "plt.ylabel(\"Embedding Similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Line Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(traces, label=\"Spectral Similarity\")\n",
    "plt.plot(score2, label=\"Embedding Similarity\")\n",
    "plt.axhline(\n",
    "    mean1,\n",
    "    color=\"r\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean Spectral Similarity: {mean1:.2f}\",\n",
    ")\n",
    "plt.axhline(\n",
    "    mean2,\n",
    "    color=\"b\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean Embedding Similarity: {mean2:.2f}\",\n",
    ")\n",
    "plt.title(\"Line Plot\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(traces, bins=30, alpha=0.5, label=\"Spectral Similarity\")\n",
    "plt.hist(score2, bins=30, alpha=0.5, label=\"Embedding Similarity\")\n",
    "plt.axvline(\n",
    "    mean1,\n",
    "    color=\"r\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean Spectral Similarity: {mean1:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    mean2,\n",
    "    color=\"b\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean Embedding Similarity: {mean2:.2f}\",\n",
    ")\n",
    "plt.title(\"Histogram\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.boxplot([traces, score2], vert=False, labels=[\"Spectral Similarity\", \"Embedding Similarity\"])\n",
    "plt.axvline(\n",
    "    mean1,\n",
    "    color=\"r\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean Spectral Similarity: {mean1:.2f}\",\n",
    ")\n",
    "plt.axvline(\n",
    "    mean2,\n",
    "    color=\"b\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=1,\n",
    "    label=f\"Mean Embedding Similarity: {mean2:.2f}\",\n",
    ")\n",
    "plt.title(\"Boxplot\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Jointplot\n",
    "g = sns.jointplot(x=traces, y=score2)\n",
    "g.ax_joint.axvline(mean1, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "g.ax_joint.axhline(mean2, color=\"b\", linestyle=\"dashed\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming ref_scores, score1, score2, score3 are your score arrays\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    normalize_values(test_snap_scores_3),\n",
    "    normalize_values(1 / traces),\n",
    "    c=test_spec_charges[::k],\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.xlabel(\"SNAP Score\")\n",
    "plt.ylabel(\"1 - Per-Sample Feature Variations\")\n",
    "plt.title(\"SNAP Score vs Per-Sample Feature Variations\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(normalize_values(test_snap_scores), normalize_values(1 - density), c=test_spec_charges[::k], alpha=0.5)\n",
    "# plt.xlabel('SNAP Score')\n",
    "# plt.ylabel('1 - Density')\n",
    "# plt.title('SNAP Score vs Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 1, 1)\n",
    "plt.scatter(test_snap_scores, consisitency)\n",
    "plt.xlabel(\"SNAP Score\")\n",
    "plt.ylabel(\"Consistency\")\n",
    "plt.title(\"SNAP Score vs Consistency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch_specs_collated = spec_collate(torch_specs)\n",
    "torch_peps_collated = pep_collate(torch_peps)\n",
    "e_specs = runModel(torch_specs_collated, snap_model, \"specs\", rank).detach().numpy()\n",
    "e_peps = runModel(torch_peps_collated, snap_model, \"peps\", rank).detach().numpy()\n",
    "\n",
    "range_val = 40  # len(e_peps)\n",
    "specs_per_pep = 15\n",
    "spec_range = 0\n",
    "for num_spec in num_specs[:range_val]:\n",
    "    spec_range += num_spec\n",
    "\n",
    "e_sp = np.concatenate((e_specs[:spec_range], e_peps[:range_val]))\n",
    "e_charges = list(map(str, test_spec_charge_list[:spec_range]))\n",
    "e_charges.extend([\"\"] * range_val)\n",
    "scaled_e_sp = StandardScaler().fit_transform(e_sp)\n",
    "\n",
    "targets = []\n",
    "for i, num_spec in enumerate(num_specs[:range_val]):\n",
    "    targets.extend([0] * num_spec)\n",
    "for i in range(range_val):\n",
    "    targets.append(1)  # + range_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(test_pep_strings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = px.data.iris()\n",
    "\n",
    "features = df.loc[:, :\"petal_width\"].to_numpy()\n",
    "# print(features)\n",
    "\n",
    "umap_2d = UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=10,\n",
    "    min_dist=1.0,\n",
    "    metric=\"euclidean\",\n",
    "    init=\"random\",\n",
    "    random_state=0,\n",
    ")\n",
    "# umap_3d = UMAP(n_components=3, n_neighbors=5, min_dist=0.1, metric='euclidean', init='random', random_state=0)\n",
    "\n",
    "proj_2d = umap_2d.fit_transform(scaled_e_sp)\n",
    "# proj_3d = umap_3d.fit_transform(scaled_e_sp)\n",
    "\n",
    "fig_2d = px.scatter(\n",
    "    proj_2d,\n",
    "    x=0,\n",
    "    y=1,\n",
    "    color=targets,\n",
    "    color_continuous_scale=px.colors.sequential.Bluered,\n",
    "    width=400,\n",
    "    height=300,\n",
    "    text=e_charges,\n",
    ")\n",
    "# fig_2d.add_scatter(x=proj_2d_peps[:,0], y=proj_2d_peps[:,1], mode='markers')\n",
    "\n",
    "# fig_3d = px.scatter_3d(\n",
    "#     proj_3d, x=0, y=1, z=2,\n",
    "#     color=targets, labels={'color': 'species'}\n",
    "# )\n",
    "\n",
    "fig_2d.update_traces(marker_size=7, textposition=\"top center\")\n",
    "fig_2d.update_layout(coloraxis_showscale=False, font={\"size\": 12})\n",
    "\n",
    "fig_2d.show()\n",
    "# fig_3d.show()\n",
    "# fig_2d.write_image(\"human_hcd_phospho-2d-20.png\")\n",
    "# fig_3d.write_image(\"human_hcd_phospho-3d-20.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "umap_2d = UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=10,\n",
    "    min_dist=1.0,\n",
    "    metric=\"euclidean\",\n",
    "    init=\"random\",\n",
    "    random_state=0,\n",
    ")\n",
    "proj_2d = umap_2d.fit_transform(scaled_e_sp)\n",
    "\n",
    "x = proj_2d[:, 0]\n",
    "y = proj_2d[:, 1]\n",
    "\n",
    "plt.figure(figsize=(12, 10), dpi=600)\n",
    "plt.grid(which=\"major\", axis=\"both\", zorder=-1.0)\n",
    "plt.rcParams.update({\"font.size\": 18})\n",
    "plt.scatter(x, y, c=targets, cmap=\"bwr\", alpha=0.8, label=\"Luck\")\n",
    "# plt.title(\"2D Map of Peptides and Spectra being Projected \")\n",
    "plt.xlabel(\"0\")\n",
    "plt.ylabel(\"1\")\n",
    "plt.savefig(\"temp.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppm(val, ppm_val):\n",
    "    return (ppm_val / 1000000) * val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_spec_batch_size = 1\n",
    "# search_spec_batch_size = 2048\n",
    "precursor_tolerance = 5\n",
    "keep_psms = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating XCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup(rank, world_size)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(rank)\n",
    "mgf_dir = config.get_config(key=\"mgf_dir\", section=\"search\")\n",
    "prep_dir = config.get_config(key=\"prep_dir\", section=\"search\")\n",
    "pep_dir = config.get_config(key=\"pep_dir\", section=\"search\")\n",
    "out_pin_dir = config.get_config(key=\"out_pin_dir\", section=\"search\")\n",
    "\n",
    "scratch_loc = \"/scratch/mtari008/job_\" + os.environ[\"SLURM_JOB_ID\"] + \"/\"\n",
    "\n",
    "mgf_dir = scratch_loc + mgf_dir\n",
    "prep_dir = scratch_loc + prep_dir\n",
    "pep_dir = scratch_loc + pep_dir\n",
    "out_pin_dir = scratch_loc + out_pin_dir\n",
    "\n",
    "if args.preprocess and args.preprocess == \"True\" and rank == 0:\n",
    "    tqdm.write(\"Preprocessing mgf files...\")\n",
    "    preprocess.preprocess_mgfs(mgf_dir, prep_dir)\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "tqdm.write(\"Reading input files...\")\n",
    "\n",
    "spec_batch_size = config.get_config(key=\"spec_batch_size\", section=\"search\")\n",
    "spec_dataset = specdataset.SpectralDataset(prep_dir)\n",
    "spec_loader = torch.utils.data.DataLoader(\n",
    "    dataset=spec_dataset, batch_size=spec_batch_size, collate_fn=dbsearch.spec_collate\n",
    ")\n",
    "\n",
    "pep_batch_size = config.get_config(key=\"pep_batch_size\", section=\"search\")\n",
    "if rank == 0:\n",
    "    pep_dataset = pepdataset.PeptideDataset(pep_dir)\n",
    "    pep_loader = torch.utils.data.DataLoader(\n",
    "        dataset=pep_dataset, batch_size=pep_batch_size, collate_fn=dbsearch.pep_collate\n",
    "    )\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "# os.environ['MASTER_ADDR'] = 'localhost'\n",
    "# os.environ['MASTER_PORT'] = '12350'\n",
    "# dist.init_process_group(backend='nccl', world_size=1, rank=0)\n",
    "# model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-randbatch-62.pt\" # 28.8k\n",
    "model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-r2r-18.pt\"  # 28.975k\n",
    "model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-r2r2r-22.pt\"\n",
    "print(\"Using model: {}\".format(model_name))\n",
    "snap_model = model.Net(vocab_size=30, embedding_dim=512, hidden_lstm_dim=512, lstm_layers=2).to(rank)\n",
    "snap_model = nn.parallel.DistributedDataParallel(snap_model, device_ids=[rank])\n",
    "# snap_model.load_state_dict(torch.load('models/32-embed-2-lstm-SnapLoss2-noch-3k-1k-152.pt')['model_state_dict'])\n",
    "# below one has 26975 identified peptides.\n",
    "# snap_model.load_state_dict(torch.load('models/512-embed-2-lstm-SnapLoss-noch-80k-nist-massive-52.pt')['model_state_dict'])\n",
    "# below one has 27.5k peps\n",
    "# snap_model.load_state_dict(torch.load('models/hcd/512-embed-2-lstm-SnapLoss2D-inputCharge-80k-nist-massive-116.pt')['model_state_dict'])\n",
    "snap_model.load_state_dict(torch.load(\"models/hcd/{}\".format(model_name))[\"model_state_dict\"])\n",
    "snap_model = snap_model.module\n",
    "snap_model.eval()\n",
    "print(snap_model)\n",
    "\n",
    "print(\"Processing spectra...\")\n",
    "e_specs = dbsearch.runModel(spec_loader, snap_model, \"specs\", rank)\n",
    "print(\"Spectra done!\")\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Processing peptides...\")\n",
    "    e_peps = dbsearch.runModel(pep_loader, snap_model, \"peps\", rank)\n",
    "    print(\"Peptides done!\")\n",
    "\n",
    "if rank == 1:\n",
    "    print(\"Processing decoys...\")\n",
    "    e_decs = dbsearch.runModel(dec_loader, snap_model, \"peps\", rank)\n",
    "    print(\"Decoys done!\")\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "search_spec_batch_size = config.get_config(key=\"search_spec_batch_size\", section=\"search\")\n",
    "if rank == 0:\n",
    "    datasets = {\"spec_dataset\": spec_dataset, \"pep_dataset\": pep_dataset}\n",
    "    embeddings = {\"e_specs\": e_specs, \"e_peps\": e_peps}\n",
    "if rank == 1:\n",
    "    datasets = {\"spec_dataset\": spec_dataset, \"dec_dataset\": dec_dataset}\n",
    "    embeddings = {\"e_specs\": e_specs, \"e_decs\": e_decs}\n",
    "search_loader = torch.utils.data.DataLoader(dataset=e_specs, batch_size=search_spec_batch_size, shuffle=False)\n",
    "\n",
    "inds, vals = dbsearch.parallel_search(search_loader, datasets, embeddings, rank)\n",
    "\n",
    "pin_charge = config.get_config(section=\"search\", key=\"charge\")\n",
    "charge_cols = [f\"charge-{ch+1}\" for ch in range(pin_charge)]\n",
    "cols = (\n",
    "    [\"SpecId\", \"Label\", \"ScanNr\", \"SNAP\", \"ExpMass\", \"CalcMass\", \"deltCn\", \"deltLCn\"]\n",
    "    + charge_cols\n",
    "    + [\"dM\", \"absdM\", \"enzInt\", \"PepLen\", \"Peptide\", \"Proteins\"]\n",
    ")\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Generating percolator pin files...\")\n",
    "    global_out = postprocess.generate_percolator_input(inds, vals, pep_dataset, spec_dataset, \"target\")\n",
    "    df = pd.DataFrame(global_out, columns=cols)\n",
    "    df.sort_values(by=\"SNAP\", inplace=True, ascending=False)\n",
    "    df.to_csv(join(out_pin_dir, \"target.pin\"), sep=\"\\t\", index=False)\n",
    "\n",
    "if rank == 1:\n",
    "    global_out = postprocess.generate_percolator_input(inds, vals, dec_dataset, spec_dataset, \"decoy\")\n",
    "    df = pd.DataFrame(global_out, columns=cols)\n",
    "    df.sort_values(by=\"SNAP\", inplace=True, ascending=False)\n",
    "    df.to_csv(join(out_pin_dir, \"decoy.pin\"), sep=\"\\t\", index=False)\n",
    "    print(\"Wrote percolator files: \\n{}\\n{}\".format(join(out_pin_dir, \"target.pin\"), join(out_pin_dir, \"decoy.pin\")))\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "    torch.cuda.set_device(rank)\n",
    "    dist.init_process_group(backend=\"nccl\", world_size=world_size, rank=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([[7, 8, 9], [10, 11, 12]])\n",
    "c = np.concatenate((a, b))\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "list3 = [50, 20, 40, 10, 30]\n",
    "\n",
    "list4 = [list1, list3]\n",
    "\n",
    "sorted_list1, sorted_list2, sorted_list3 = zip(*sorted(zip(list1, list2, list3), key=lambda x: x[2]))\n",
    "\n",
    "print(sorted_list1)  # (4, 2, 5, 3, 1)\n",
    "print(sorted_list2)  # ('d', 'b', 'e', 'c', 'a')\n",
    "print(sorted_list3)  # (10, 20, 30, 40, 50)\n",
    "\n",
    "print(np.concatenate(list4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
