{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../DeepAtles')\n",
    "import read_spectra\n",
    "\n",
    "sys.path.append('../../DeepAtles/src')\n",
    "from src.atlesconfig import config\n",
    "\n",
    "config.PARAM_PATH = '../config.ini'\n",
    "\n",
    "import h5py\n",
    "import re\n",
    "from pathlib import Path, PurePath\n",
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "from os.path import join\n",
    "import time\n",
    "\n",
    "import progressbar\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from src.atlesconfig import config\n",
    "from src.atlesutils import simulatespectra as sim\n",
    "from src.atlespredict import (dbsearch, pepdataset, postprocess, preprocess, specdataset, specollate_model)\n",
    "from src.atlestrain import dataset, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PepInfo:\n",
    "    pep_list: list\n",
    "    prot_list: list\n",
    "    pep_mass_list: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = str(config.get_config(key=\"master_port\", section=\"input\"))\n",
    "    torch.cuda.set_device(rank)\n",
    "    dist.init_process_group(backend='nccl', world_size=world_size, rank=rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n"
     ]
    }
   ],
   "source": [
    "out_pin_dir = config.get_config(key=\"out_pin_dir\", section=\"search\")\n",
    "pep_dir = config.get_config(key=\"pep_dir\", section=\"search\")\n",
    "pep_index_name = PurePath(pep_dir).name\n",
    "print(pep_index_name)\n",
    "index_path = join(config.get_config(key=\"index_path\", section=\"search\"), pep_index_name)\n",
    "min_pep_len = config.get_config(key=\"min_pep_len\", section=\"ml\")\n",
    "max_pep_len = config.get_config(key=\"max_pep_len\", section=\"ml\")\n",
    "max_clvs = config.get_config(key=\"max_clvs\", section=\"ml\")\n",
    "\n",
    "length_filter = config.get_config(key=\"length_filter\", section=\"filter\")\n",
    "len_tol_pos = config.get_config(key=\"len_tol_pos\", section=\"filter\") if length_filter else 0\n",
    "len_tol_neg = config.get_config(key=\"len_tol_neg\", section=\"filter\") if length_filter else 0\n",
    "missed_cleavages_filter = config.get_config(key=\"missed_cleavages_filter\", section=\"filter\")\n",
    "modification_filter = config.get_config(key=\"modification_filter\", section=\"filter\")\n",
    "\n",
    "pep_batch_size = config.get_config(key=\"pep_batch_size\", section=\"search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if preprocessed folder exisits. \n",
    "# if not do step: 1 - 2\n",
    "# do step 3 - 4\n",
    "# \n",
    "# Step 1 \n",
    "def classify_peptides(rank):\n",
    "    pep_dataset = pepdataset.PeptideDataset(pep_dir, decoy=rank == 1)\n",
    "    os.mkdir(index_path)\n",
    "    pep_classes_path = join(index_path, 'peptide_classes' if rank == 0 else 'decoy_classes')\n",
    "    os.mkdir(pep_classes_path)\n",
    "    os.mkdir(join(index_path, 'peptide_embeddings' if rank == 0 else 'decoy_embeddings'))\n",
    "\n",
    "    # 1 - classify peptides and write to 144 separate files.\n",
    "    print('Opening files')\n",
    "    open_files = {}\n",
    "    class_offsets = {}\n",
    "    for length, clv, mod in itertools.product(range(min_pep_len, max_pep_len + 1), range(max_clvs + 1), range(2)):\n",
    "        file_name = '{}-{}-{}'.format(length, clv, mod)\n",
    "        open_files[file_name] = open(join(pep_classes_path, file_name), 'a')\n",
    "        class_offsets[file_name] = 0\n",
    "\n",
    "    print('Classifying peptides and writing to files')\n",
    "    for idx, (pep, clv, mod, prot) in enumerate(zip(\n",
    "            pep_dataset.pep_list, pep_dataset.missed_cleavs, pep_dataset.pep_modified_list, pep_dataset.prot_list)):\n",
    "        pep_len = sum(map(str.isupper, pep))\n",
    "        if min_pep_len <= pep_len <= max_pep_len and 0 <= clv <= max_clvs:\n",
    "            file_name = '{}-{}-{}'.format(int(pep_len), int(clv), int(mod))\n",
    "            if file_name in open_files:\n",
    "                f = open_files[file_name]\n",
    "                f.write('>' + prot + '\\n')\n",
    "                f.write(pep + '\\n')\n",
    "                class_offsets[file_name] += 1\n",
    "\n",
    "    print('Closing files')\n",
    "    for _, f in open_files.items():\n",
    "        f.close()\n",
    "    \n",
    "    cum = 0\n",
    "    for length, clv, mod in itertools.product(range(min_pep_len, max_pep_len + 1), range(max_clvs + 1), range(2)):\n",
    "        file_name = '{}-{}-{}'.format(length, clv, mod)\n",
    "        offset = class_offsets[file_name]\n",
    "        class_offsets[file_name] = cum\n",
    "        cum += offset\n",
    "\n",
    "    pickle.dump(class_offsets, open(\n",
    "        join(index_path, '{}'.format(\n",
    "            'peptide_class_offsets.pkl' if rank == 0 else 'decoy_class_offsets.pkl')), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snap_model(rank):\n",
    "    model_name = config.get_config(key=\"model_name\", section=\"search\")\n",
    "    print(\"Using model: {}\".format(model_name))\n",
    "    snap_model = specollate_model.Net(vocab_size=30, embedding_dim=512, hidden_lstm_dim=512, lstm_layers=2).to(rank)\n",
    "    snap_model = nn.parallel.DistributedDataParallel(snap_model, device_ids=[rank])\n",
    "    # snap_model.load_state_dict(torch.load('models/32-embed-2-lstm-SnapLoss2-noch-3k-1k-152.pt')['model_state_dict'])\n",
    "    # below one has 26975 identified peptides.\n",
    "    # snap_model.load_state_dict(torch.load('models/512-embed-2-lstm-SnapLoss-noch-80k-nist-massive-52.pt')['model_state_dict'])\n",
    "    # below one has 27.5k peps\n",
    "    # snap_model.load_state_dict(torch.load('models/hcd/512-embed-2-lstm-SnapLoss2D-inputCharge-80k-nist-massive-116.pt')['model_state_dict'])\n",
    "    snap_model.load_state_dict(torch.load('../specollate-model/{}'.format(model_name))['model_state_dict'])\n",
    "    snap_model = snap_model.module\n",
    "    snap_model.eval()\n",
    "    print(snap_model)\n",
    "    return snap_model\n",
    "\n",
    "# dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - load each class and process it using specollate, save embeddings for each class separately\n",
    "def process_peptides(rank, snap_model):\n",
    "    for length, clv, mod in itertools.product(range(min_pep_len, max_pep_len + 1), range(max_clvs + 1), range(2)):\n",
    "        file_name = '{}-{}-{}'.format(length, clv, mod)\n",
    "        pep_classes_path = join(index_path, 'peptide_classes' if rank == 0 else 'decoy_classes')\n",
    "        pep_file_path = join(pep_classes_path, file_name)\n",
    "        if os.path.exists(pep_file_path):\n",
    "            print('Processing file: {}'.format(file_name))\n",
    "            # process peptides\n",
    "            pep_dataset = pepdataset.PeptideDataset(pep_dir, pep_file_path, decoy=rank == 1)\n",
    "            pep_loader = torch.utils.data.DataLoader(\n",
    "                dataset=pep_dataset, batch_size=pep_batch_size,\n",
    "                collate_fn=dbsearch.pep_collate)\n",
    "            \n",
    "            print(\"Processing {}...\".format(\"Peptides\" if rank == 0 else \"Decoys\"))\n",
    "            e_peps = dbsearch.runSpeCollateModel(pep_loader, snap_model, \"peps\", rank)\n",
    "            print(\"Peptides done!\")\n",
    "\n",
    "            # save embeddings\n",
    "            print('Saving embeddings at {}'.format(join(index_path, '{}'.format(\n",
    "                'peptide_embeddings' if rank == 0 else 'decoy_embeddings'), file_name)))\n",
    "            torch.save(e_peps, join(index_path, '{}'.format('peptide_embeddings' if rank == 0 else 'decoy_embeddings'), file_name))\n",
    "            print('Done \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_atles(rank, world_size, spec_loader):\n",
    "    model_ = model.Net().to(rank)\n",
    "    model_ = nn.parallel.DistributedDataParallel(model_, device_ids=[rank])\n",
    "    # model_.load_state_dict(torch.load('atles-out/16403437/models/pt-mass-ch-16403437-1toz70vi-472.pt')['model_state_dict'])\n",
    "    # model_.load_state_dict(torch.load(\n",
    "    #     '/lclhome/mtari008/DeepAtles/atles-out/123/models/pt-mass-ch-123-2zgb2ei9-385.pt'\n",
    "    #     )['model_state_dict'])\n",
    "    model_.load_state_dict(torch.load(\n",
    "        '/lclhome/mtari008/DeepAtles/atles-out/1382/models/nist-massive-deepnovo-mass-ch-1382-c8mlqbq7-157.pt'\n",
    "    )['model_state_dict'])\n",
    "    model_ = model_.module\n",
    "    model_.eval()\n",
    "    print(model_)\n",
    "\n",
    "    lens, cleavs, mods = dbsearch.runAtlesModel(spec_loader, model_, rank)\n",
    "    pred_cleavs_softmax = torch.log_softmax(cleavs, dim=1)\n",
    "    _, pred_cleavs = torch.max(pred_cleavs_softmax, dim=1)\n",
    "    pred_mods_softmax = torch.log_softmax(mods, dim=1)\n",
    "    _, pred_mods = torch.max(pred_mods_softmax, dim=1)\n",
    "\n",
    "    return (\n",
    "        torch.round(lens).type(torch.IntTensor).squeeze().tolist(),\n",
    "        pred_cleavs.squeeze().tolist(),\n",
    "        pred_mods.squeeze().tolist()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_spectra(rank, snap_model):\n",
    "    prep_path = config.get_config(section='search', key='prep_path')\n",
    "    spec_batch_size = config.get_config(key=\"spec_batch_size\", section=\"search\")\n",
    "    spec_dataset = specdataset.SpectraDataset(join(prep_path, \"specs.pkl\"))\n",
    "    spec_loader = torch.utils.data.DataLoader(\n",
    "        dataset=spec_dataset, batch_size=spec_batch_size,\n",
    "        collate_fn=dbsearch.spec_collate)\n",
    "\n",
    "    print(\"Processing spectra...\")\n",
    "    e_specs = dbsearch.runSpeCollateModel(spec_loader, snap_model, \"specs\", rank)\n",
    "    print(\"Spectra done!\")\n",
    "\n",
    "    atles_start_time = time.time()\n",
    "    lens, cleavs, mods = run_atles(rank, 1, spec_loader)\n",
    "    atles_end_time = time.time()\n",
    "    atles_time = atles_end_time - atles_start_time\n",
    "    print(\"Atles time: {}\".format(atles_time))\n",
    "    return e_specs, lens, cleavs, mods, spec_dataset.masses, spec_dataset.charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Loop over spectra classes, load embeddings for peptides, peform db search\n",
    "def create_spectra_dict(lens, cleavs, mods, e_specs, spec_masses):\n",
    "    print(\"Creating spectra filtered dictionary.\")\n",
    "    spec_filt_dict = defaultdict(list)\n",
    "    for idx, (l, clv, mod) in enumerate(zip(lens, cleavs, mods)):\n",
    "        if min_pep_len <= l <= max_pep_len and 0 <= clv <= max_clvs:\n",
    "            key = '{}-{}-{}'.format(int(l), int(clv), int(mod))\n",
    "            # FIXME: needs to add actual spectra embeddings\n",
    "            spec_filt_dict[key].append([idx, e_specs[idx], spec_masses[idx]])\n",
    "\n",
    "    return spec_filt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_pin(rank, pep_inds, psm_vals, spec_inds, l_pep_dataset, spec_charges, cols):\n",
    "    if rank == 0:\n",
    "        print(\"Generating percolator pin files...\")\n",
    "    global_out = postprocess.generate_percolator_input(\n",
    "        pep_inds, psm_vals, spec_inds, l_pep_dataset, spec_charges, \"target\" if rank == 0 else \"decoy\")\n",
    "    df = pd.DataFrame(global_out, columns=cols)\n",
    "    df.sort_values(by=\"SNAP\", inplace=True, ascending=False)\n",
    "    with open(join(out_pin_dir, \"target.pin\" if rank == 0 else \"decoy.pin\"), 'a') as f:\n",
    "        df.to_csv(f, sep=\"\\t\", index=False, header=not f.tell())\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"Wrote percolator files: \")\n",
    "    # dist.barrier()\n",
    "    print(\"{}\".format(join(out_pin_dir, \"target.pin\") if rank == 0 else join(out_pin_dir, \"decoy.pin\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_database(rank, spec_filt_dict, spec_charges):\n",
    "    search_spec_batch_size = config.get_config(key=\"search_spec_batch_size\", section=\"search\")\n",
    "    # dist.barrier()\n",
    "\n",
    "    if rank == 0:\n",
    "        search_start_time = time.time()\n",
    "    # Run database search for each dict item\n",
    "    unfiltered_time = 0\n",
    "\n",
    "    class_offsets = pickle.load(\n",
    "        open(join(index_path, '{}'.format(\n",
    "            'peptide_class_offsets.pkl' if rank == 0 else 'decoy_class_offsets.pkl')), 'rb'))\n",
    "\n",
    "    pin_charge = config.get_config(section=\"search\", key=\"charge\")\n",
    "    charge_cols = [f\"charge-{ch+1}\" for ch in range(pin_charge)]\n",
    "    cols = [\"SpecId\", \"Label\", \"ScanNr\", \"SNAP\", \"ExpMass\", \"CalcMass\", \"deltCn\",\n",
    "            \"deltLCn\"] + charge_cols + [\"dM\", \"absdM\", \"enzInt\", \"PepLen\", \"Peptide\", \"Proteins\"]\n",
    "            \n",
    "    print(\"Running filtered {} database search.\".format(\"target\" if rank == 0 else \"decoy\"))\n",
    "    for key in spec_filt_dict:\n",
    "        print('Searching for key {}.'.format(key))\n",
    "        spec_inds = []\n",
    "        pep_inds = []\n",
    "        psm_vals = []\n",
    "        offset = 0\n",
    "        pep_info = PepInfo([], [], [])\n",
    "        for tol in range(len_tol_neg, len_tol_pos + 1):\n",
    "            key_len, key_clv, key_mod = int(key.split('-')[0]), int(key.split('-')[1]), int(key.split('-')[2])\n",
    "            file_name = '{}-{}-{}'.format(key_len + tol, key_clv, key_mod)\n",
    "            pep_classes_path = join(index_path, 'peptide_classes' if rank == 0 else 'decoy_classes')\n",
    "            pep_file_path = join(pep_classes_path, file_name)\n",
    "            if not os.path.exists(pep_file_path):\n",
    "                print(\"Key {} not found in pep_dataset\".format(pep_file_path))\n",
    "                continue\n",
    "            print('Processing file: {}'.format(file_name))\n",
    "            # process peptides\n",
    "            pep_dataset = pepdataset.PeptideDataset(pep_dir, pep_file_path, decoy=rank == 1)\n",
    "            pep_loader = torch.utils.data.DataLoader(\n",
    "                dataset=pep_dataset, batch_size=pep_batch_size,\n",
    "                collate_fn=dbsearch.pep_collate)\n",
    "            pep_info.pep_list += pep_dataset.pep_list\n",
    "            pep_info.prot_list += pep_dataset.prot_list\n",
    "            pep_info.pep_mass_list += pep_dataset.pep_mass_list\n",
    "\n",
    "            # load embeddings\n",
    "            pep_embeddings_path = join(index_path, 'peptide_embeddings' if rank == 0 else 'decoy_embeddings')\n",
    "            embedding_file_path = join(pep_embeddings_path, file_name)\n",
    "            e_peps = torch.load(embedding_file_path)\n",
    "            # pep_data = [[idx + class_offsets[file_name], e_pep, mass] \\\n",
    "            #     for idx, (e_pep, mass) in enumerate(zip(e_peps, pep_dataset.pep_mass_list))]\n",
    "            pep_data = [[idx + offset, e_pep, mass] \\\n",
    "                for idx, (e_pep, mass) in enumerate(zip(e_peps, pep_dataset.pep_mass_list))]\n",
    "\n",
    "            print(\"Searching against key {} with {} peptides.\".format(file_name, len(pep_dataset.pep_mass_list)))\n",
    "            spec_subset = spec_filt_dict[key]\n",
    "            search_loader = torch.utils.data.DataLoader(\n",
    "                dataset=spec_subset, num_workers=0, batch_size=search_spec_batch_size, shuffle=False)\n",
    "            unfiltered_start_time = time.time()\n",
    "            l_spec_inds, l_pep_inds, l_psm_vals = dbsearch.filtered_parallel_search(\n",
    "                search_loader, pep_data, rank)\n",
    "            unfiltered_time += time.time() - unfiltered_start_time\n",
    "\n",
    "            if not l_spec_inds:\n",
    "                continue\n",
    "            spec_inds.extend(l_spec_inds)\n",
    "            pep_inds.append(l_pep_inds)\n",
    "            psm_vals.append(l_psm_vals)\n",
    "\n",
    "            offset += len(pep_dataset.pep_mass_list)\n",
    "\n",
    "        # if not l_spec_inds:\n",
    "        #     continue\n",
    "        # spec_inds.extend(l_spec_inds)\n",
    "        # pep_inds.append(l_pep_inds)\n",
    "        # psm_vals.append(l_psm_vals)\n",
    "\n",
    "        pep_inds = torch.cat(pep_inds, 0)\n",
    "        psm_vals = torch.cat(psm_vals, 0)\n",
    "\n",
    "        print(\"{} PSMS: {}\".format(\"Target\" if rank == 0 else \"Decoy\", len(pep_inds)))\n",
    "\n",
    "        # 4 - Write PSMs to pin file\n",
    "        write_to_pin(pep_inds, psm_vals, spec_inds, pep_info, spec_charges, cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_atles_search(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    if not os.path.exists(index_path):\n",
    "        classify_peptides(rank)\n",
    "        process_peptides(rank)\n",
    "\n",
    "    model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-r2r-18.pt\"  # 28.975k\n",
    "    model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-r2r2r-22.pt\"\n",
    "    snap_model = get_snap_model(rank)\n",
    "    e_specs, lens, cleavs, mods, spec_masses, spec_charges = process_spectra(rank, snap_model)\n",
    "    spec_filt_dict = create_spectra_dict(lens, cleavs, mods, e_specs, spec_masses)\n",
    "    search_database(rank, spec_filt_dict, spec_charges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/lclhome/mtari008/anaconda3/envs/deepatles/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/lclhome/mtari008/anaconda3/envs/deepatles/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "  File \"/lclhome/mtari008/anaconda3/envs/deepatles/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'run_atles_search' on <module '__main__' (built-in)>\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/lclhome/mtari008/anaconda3/envs/deepatles/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'run_atles_search' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mp\u001b[39m.\u001b[39;49mspawn(run_atles_search, args\u001b[39m=\u001b[39;49m(\u001b[39m2\u001b[39;49m,), nprocs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, join\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepatles/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:240\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    236\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    238\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m start_method)\n\u001b[1;32m    239\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepatles/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    199\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepatles/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:149\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[39mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprocess \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with signal \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    142\u001b[0m             (error_index, name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m             signal_name\u001b[39m=\u001b[39mname\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[39mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    150\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprocess \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with exit code \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    151\u001b[0m             (error_index, exitcode),\n\u001b[1;32m    152\u001b[0m             error_index\u001b[39m=\u001b[39merror_index,\n\u001b[1;32m    153\u001b[0m             error_pid\u001b[39m=\u001b[39mfailed_process\u001b[39m.\u001b[39mpid,\n\u001b[1;32m    154\u001b[0m             exit_code\u001b[39m=\u001b[39mexitcode\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m original_trace \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_queues[error_index]\u001b[39m.\u001b[39mget()\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "mp.spawn(run_atles_search, args=(2,), nprocs=2, join=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepatles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00151e0d58dec85c9023f7cf2cb52563442e50bc5bc18cc73647572af3f1d275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
