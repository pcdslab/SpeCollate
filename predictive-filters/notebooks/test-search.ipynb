{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from os.path import join, dirname\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import sys\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../../DeepAtles')\n",
    "import run_search\n",
    "\n",
    "sys.path.append('../../DeepAtles/src')\n",
    "\n",
    "from src.atlesconfig import config\n",
    "from src.atlestrain import dataset, model\n",
    "from src.atlespredict import dbsearch, specdataset, pepdataset, preprocess, postprocess, specollate_model\n",
    "config.PARAM_PATH = '../config.ini'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_atles(rank, world_size, spec_loader):\n",
    "    model_ = model.Net().to(rank)\n",
    "    model_ = nn.parallel.DistributedDataParallel(model_, device_ids=[rank])\n",
    "    # model_.load_state_dict(torch.load('atles-out/16403437/models/pt-mass-ch-16403437-1toz70vi-472.pt')['model_state_dict'])\n",
    "    # model_.load_state_dict(torch.load(\n",
    "    #     '/lclhome/mtari008/DeepAtles/atles-out/123/models/pt-mass-ch-123-2zgb2ei9-385.pt')['model_state_dict'])\n",
    "    model_.load_state_dict(torch.load(\n",
    "        '/lclhome/mtari008/DeepAtles/atles-out/1382/models/nist-massive-deepnovo-mass-ch-1382-c8mlqbq7-157.pt'\n",
    "    )['model_state_dict'])\n",
    "    model_ = model_.module\n",
    "    model_.eval()\n",
    "    print(model_)\n",
    "\n",
    "    lens, cleavs, mods = dbsearch.runAtlesModel(spec_loader, model_, rank)\n",
    "\n",
    "    pred_cleavs_softmax = torch.log_softmax(cleavs, dim=1)\n",
    "    _, pred_cleavs = torch.max(pred_cleavs_softmax, dim=1)\n",
    "    pred_mods_softmax = torch.log_softmax(mods, dim=1)\n",
    "    _, pred_mods = torch.max(pred_mods_softmax, dim=1)\n",
    "\n",
    "    return (\n",
    "        torch.round(lens).type(torch.IntTensor).squeeze().tolist(),\n",
    "        pred_cleavs.squeeze().tolist(),\n",
    "        pred_mods.squeeze().tolist()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12345'\n",
    "    torch.cuda.set_device(rank)\n",
    "    dist.init_process_group(backend='nccl', world_size=world_size, rank=rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "setup(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(rank)\n",
    "mgf_dir = config.get_config(key=\"mgf_dir\", section=\"search\")\n",
    "prep_dir = config.get_config(key=\"prep_dir\", section=\"search\")\n",
    "pep_dir = config.get_config(key=\"pep_dir\", section=\"search\")\n",
    "out_pin_dir = config.get_config(key=\"out_pin_dir\", section=\"search\")\n",
    "\n",
    "# scratch_loc = \"/scratch/mtari008/job_\" + os.environ['SLURM_JOB_ID'] + \"/\"\n",
    "\n",
    "# mgf_dir     = scratch_loc + mgf_dir\n",
    "# prep_dir    = scratch_loc + prep_dir\n",
    "# pep_dir     = scratch_loc + pep_dir\n",
    "# out_pin_dir = scratch_loc + out_pin_dir\n",
    "\n",
    "if rank == 0:\n",
    "    tqdm.write(\"Reading input files...\")\n",
    "\n",
    "batch_size = config.get_config(section=\"ml\", key=\"batch_size\")\n",
    "prep_path = config.get_config(section='search', key='prep_path')\n",
    "spec_batch_size = config.get_config(key=\"spec_batch_size\", section=\"search\")\n",
    "spec_dataset = specdataset.SpectraDataset(join(prep_path, \"specs.pkl\"))\n",
    "spec_loader = torch.utils.data.DataLoader(\n",
    "    dataset=spec_dataset, batch_size=spec_batch_size,\n",
    "    collate_fn=dbsearch.spec_collate)\n",
    "\n",
    "pep_batch_size = config.get_config(key=\"pep_batch_size\", section=\"search\")\n",
    "\n",
    "pep_dataset = pepdataset.PeptideDataset(pep_dir, decoy=rank == 1)\n",
    "pep_loader = torch.utils.data.DataLoader(\n",
    "    dataset=pep_dataset, batch_size=pep_batch_size,\n",
    "    collate_fn=dbsearch.pep_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens, cleavs, mods = run_atles(rank, 1, spec_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['MASTER_ADDR'] = 'localhost'\n",
    "# os.environ['MASTER_PORT'] = '12350'\n",
    "# dist.init_process_group(backend='nccl', world_size=1, rank=0)\n",
    "# model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-randbatch-62.pt\" # 28.8k\n",
    "model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-r2r-18.pt\"  # 28.975k\n",
    "model_name = \"512-embed-2-lstm-SnapLoss2D-80k-nist-massive-no-mc-semi-r2r2r-22.pt\"\n",
    "print(\"Using model: {}\".format(model_name))\n",
    "snap_model = specollate_model.Net(vocab_size=30, embedding_dim=512, hidden_lstm_dim=512, lstm_layers=2).to(rank)\n",
    "snap_model = nn.parallel.DistributedDataParallel(snap_model, device_ids=[rank])\n",
    "# snap_model.load_state_dict(torch.load('models/32-embed-2-lstm-SnapLoss2-noch-3k-1k-152.pt')['model_state_dict'])\n",
    "# below one has 26975 identified peptides.\n",
    "# snap_model.load_state_dict(torch.load('models/512-embed-2-lstm-SnapLoss-noch-80k-nist-massive-52.pt')['model_state_dict'])\n",
    "# below one has 27.5k peps\n",
    "# snap_model.load_state_dict(torch.load('models/hcd/512-embed-2-lstm-SnapLoss2D-inputCharge-80k-nist-massive-116.pt')['model_state_dict'])\n",
    "snap_model.load_state_dict(torch.load('../specollate-model/{}'.format(model_name))['model_state_dict'])\n",
    "snap_model = snap_model.module\n",
    "snap_model.eval()\n",
    "print(snap_model)\n",
    "\n",
    "print(\"Processing spectra...\")\n",
    "e_specs = dbsearch.runSpeCollateModel(spec_loader, snap_model, \"specs\", rank)\n",
    "print(\"Spectra done!\")\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "print(\"Processing {}...\".format(\"Peptides\" if rank == 0 else \"Decoys\"))\n",
    "e_peps = dbsearch.runSpeCollateModel(pep_loader, snap_model, \"peps\", rank)\n",
    "print(\"Peptides done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.atlestrain import process\n",
    "\n",
    "def ppm(val, ppm_val):\n",
    "    return (ppm_val / 1000000.0) * val\n",
    "\n",
    "\n",
    "def spec_collate(batch):\n",
    "    specs = torch.cat([item[0] for item in batch], 0)\n",
    "    char_mass = torch.FloatTensor([item[1] for item in batch])\n",
    "    return [specs, char_mass]\n",
    "\n",
    "\n",
    "def pep_collate(batch):\n",
    "    peps = torch.stack([item for item in batch], 0)\n",
    "    dummy_spec = np.zeros(config.get_config(section=\"input\", key=\"spec_size\"))\n",
    "    dummy_spec = torch.from_numpy(dummy_spec).float().unsqueeze(0)\n",
    "    dummy_pep = np.zeros((2, config.get_config(section=\"ml\", key=\"pep_seq_len\") + 24))\n",
    "    dummy_pep = torch.from_numpy(dummy_pep).long()  # .unsqueeze(0)\n",
    "    # tqdm.write(\"{}\".format(peps.shape))\n",
    "    # tqdm.write(\"{}\".format(dummy_pep.shape))\n",
    "    return [dummy_spec, peps, dummy_pep]\n",
    "\n",
    "\n",
    "def get_search_mask(spec_masses, pep_masses, tol):\n",
    "    l_tol = tol\n",
    "    rows = []\n",
    "    cols = []\n",
    "    pep_min = pep_max = 0\n",
    "    for row_id, spec_mass in enumerate(spec_masses):\n",
    "        # min_mass = max(spec_mass - l_tol, 0.0)\n",
    "        # max_mass = spec_mass + l_tol\n",
    "        min_mass = max(spec_mass - ppm(spec_mass, l_tol), 0.0)\n",
    "        max_mass = spec_mass + ppm(spec_mass, l_tol)\n",
    "        while (pep_min < len(pep_masses) and min_mass > pep_masses[pep_min]):\n",
    "            pep_min += 1\n",
    "        while (pep_max < len(pep_masses) and max_mass > pep_masses[pep_max]):\n",
    "            pep_max += 1\n",
    "        # pep_min = max(pep_min - 1, 0)\n",
    "        # pep_max = min(pep_max + 1, len(pep_masses) - 1)\n",
    "\n",
    "        # if pep_max == pep_min:\n",
    "        #     print(row_id, pep_max, pep_min)\n",
    "        rows.extend([row_id] * (pep_max - pep_min))\n",
    "        cols.extend(range(pep_min, pep_max))\n",
    "\n",
    "    assert len(rows) == len(cols)\n",
    "    mask = torch.zeros(len(spec_masses), len(pep_masses))\n",
    "    mask[rows, cols] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def filtered_parallel_search(search_loader, peps, rank):\n",
    "    spec_inds = []\n",
    "    sort_inds = []\n",
    "    sort_vals = []\n",
    "\n",
    "    keep_psms = config.get_config(key=\"keep_psms\", section=\"search\")\n",
    "    precursor_tolerance = config.get_config(key=\"precursor_tolerance\", section=\"search\")\n",
    "\n",
    "    pbar = tqdm(search_loader, file=sys.stdout)\n",
    "    pbar.set_description('Running Database Search...')\n",
    "    # with progressbar.ProgressBar(max_value=len(search_loader)) as bar:\n",
    "    for idx, [spec_idx, spec_batch, spec_masses] in enumerate(pbar):\n",
    "        l_tol = precursor_tolerance\n",
    "        # min_mass = max(spec_masses[0] - l_tol, 0)\n",
    "        # max_mass = spec_masses[-1] + l_tol\n",
    "        min_mass = max(spec_masses[0] - ppm(spec_masses[0], l_tol), 0)\n",
    "        max_mass = spec_masses[-1] + ppm(spec_masses[-1], l_tol)\n",
    "\n",
    "        pep_min = pep_max = 0\n",
    "        while (pep_min < len(peps) and\n",
    "                min_mass - peps[pep_min][2] > 0.001):\n",
    "            pep_min += 1\n",
    "        while (pep_max < len(peps) and\n",
    "                max_mass - peps[pep_max][2] >= 0.001):\n",
    "            pep_max += 1\n",
    "\n",
    "        pep_batch = peps[pep_min:pep_max]\n",
    "        if len(pep_batch) == 0 or pep_min == pep_max:\n",
    "            continue\n",
    "        pep_masses = []\n",
    "\n",
    "        spec_batch = spec_batch.to(rank)\n",
    "        # print(\"pep batch len: {}\".format(len(pep_batch)))\n",
    "        l_pep_batch_size = 16384\n",
    "        # l_pep_batch_size = 32768\n",
    "        pep_loader = torch.utils.data.DataLoader(\n",
    "            dataset=pep_batch, batch_size=l_pep_batch_size, shuffle=False)\n",
    "        l_pep_dist = []\n",
    "        g_ids = []\n",
    "        for g_idx, l_pep_batch, l_pep_masses in pep_loader:\n",
    "            g_ids.extend(g_idx)\n",
    "            pep_masses.extend(l_pep_masses)\n",
    "            l_pep_batch = l_pep_batch.to(rank)\n",
    "            # spec_pep_mask = get_search_mask(spec_masses, l_pep_masses, precursor_tolerance).to(rank)\n",
    "            # spec_pep_mask[spec_pep_mask == 0] = float(\"inf\")\n",
    "            spec_pep_dist = 1.0 / process.pairwise_distances(spec_batch, l_pep_batch).to(\"cpu\")\n",
    "            l_pep_dist.append(spec_pep_dist)\n",
    "        # print(len(pep_batch))\n",
    "        # print(len(g_ids))\n",
    "        if len(g_ids) < keep_psms + 1:\n",
    "            g_ids.extend([g_ids[0]] * (keep_psms + 1 - len(g_ids)))\n",
    "        g_ids = torch.IntTensor(g_ids)\n",
    "        # print(g_ids.shape)\n",
    "        if not l_pep_dist:\n",
    "            continue\n",
    "        pep_sort = torch.cat(l_pep_dist, 1)\n",
    "        spec_pep_mask = get_search_mask(spec_masses, pep_masses, precursor_tolerance)\n",
    "        pep_sort = (pep_sort * spec_pep_mask)\n",
    "        pep_sort = torch.cat((pep_sort, torch.zeros(len(spec_batch), keep_psms + 1)), axis=1)\n",
    "        pep_lcn = np.ma.masked_array(pep_sort, mask=pep_sort == 0).min(1).data\n",
    "        pep_sort = pep_sort.sort(descending=True, stable=True)\n",
    "        spec_inds.extend(spec_idx)\n",
    "        # no need to offset as g_ids is constructed for pep_batch.\n",
    "        sort_inds.append(g_ids[pep_sort.indices[:, :keep_psms + 1]])\n",
    "        sort_vals.append(torch.cat((pep_sort.values[:, :keep_psms + 1],\n",
    "                                    torch.from_numpy(pep_lcn).unsqueeze(1)), 1))\n",
    "\n",
    "        # bar.update(idx)\n",
    "    if not spec_inds:\n",
    "        return None, None, None\n",
    "    pep_inds = torch.cat(sort_inds, 0)\n",
    "    pep_vals = torch.cat(sort_vals, 0)\n",
    "    return spec_inds, pep_inds, pep_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pep_len = config.get_config(key=\"min_pep_len\", section=\"ml\")\n",
    "max_pep_len = config.get_config(key=\"max_pep_len\", section=\"ml\")\n",
    "max_clvs = config.get_config(key=\"max_clvs\", section=\"ml\")\n",
    "spec_dataset.filt_dict = defaultdict(list)\n",
    "print(\"Creating spectra filtered dictionary.\")\n",
    "for idx, (l, clv, mod, e_spec, spec_mass) in enumerate(zip(lens, cleavs, mods, e_specs, spec_dataset.masses)):\n",
    "    if min_pep_len <= l <= max_pep_len and 0 <= clv <= max_clvs:\n",
    "        key = '{}-{}-{}'.format(l, clv, int(mod))\n",
    "        spec_dataset.filt_dict[key].append([idx, e_spec, spec_mass])\n",
    "\n",
    "pep_batch_size = config.get_config(key=\"pep_batch_size\", section=\"search\")\n",
    "####### rank==1 decides whether to search against decoy database #######\n",
    "pep_dataset.filt_dict = defaultdict(list)\n",
    "print(\"Creating {} peptide filtered dictionary.\".format(\"target\" if rank == 0 else \"decoy\"))\n",
    "for idx, (pep, clv, mod, e_pep, pep_mass) in enumerate(zip(\n",
    "        pep_dataset.pep_list, pep_dataset.missed_cleavs, pep_dataset.pep_modified_list, e_peps, pep_dataset.pep_mass_list)):\n",
    "    pep_len = sum(map(str.isupper, pep))\n",
    "    if min_pep_len <= pep_len <= max_pep_len and 0 <= clv <= max_clvs:\n",
    "        key = '{}-{}-{}'.format(pep_len, clv, int(mod))\n",
    "        pep_dataset.filt_dict[key].append([idx, e_pep, pep_mass])\n",
    "\n",
    "search_spec_batch_size = config.get_config(key=\"search_spec_batch_size\", section=\"search\")\n",
    "# Run database search for each dict item\n",
    "spec_inds = []\n",
    "pep_inds = []\n",
    "psm_vals = []\n",
    "print(\"Running filtered {} database search.\".format(\"target\" if rank == 0 else \"decoy\"))\n",
    "for key in spec_dataset.filt_dict:\n",
    "    if key not in pep_dataset.filt_dict:\n",
    "        print(\"Key {} not found in pep_dataset\".format(key))\n",
    "        continue\n",
    "    print(\"Key {} found. {} peptides in pep_dataset\".format(key, len(pep_dataset.filt_dict[key])))\n",
    "    spec_subset = spec_dataset.filt_dict[key]\n",
    "    search_loader = torch.utils.data.DataLoader(\n",
    "        dataset=spec_subset, num_workers=0, batch_size=search_spec_batch_size, shuffle=False)\n",
    "\n",
    "    l_spec_inds, l_pep_inds, l_psm_vals = filtered_parallel_search(\n",
    "        search_loader, pep_dataset.filt_dict[key], rank)\n",
    "    if l_spec_inds is None:\n",
    "        continue\n",
    "    spec_inds.extend(l_spec_inds)\n",
    "    pep_inds.append(l_pep_inds)\n",
    "    psm_vals.append(l_psm_vals)\n",
    "pep_inds = torch.cat(pep_inds, 0)\n",
    "psm_vals = torch.cat(psm_vals, 0)\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "pin_charge = config.get_config(section=\"search\", key=\"charge\")\n",
    "charge_cols = [f\"charge-{ch+1}\" for ch in range(pin_charge)]\n",
    "cols = [\"SpecId\", \"Label\", \"ScanNr\", \"SNAP\", \"ExpMass\", \"CalcMass\", \"deltCn\",\n",
    "        \"deltLCn\"] + charge_cols + [\"dM\", \"absdM\", \"enzInt\", \"PepLen\", \"Peptide\", \"Proteins\"]\n",
    "\n",
    "dist.barrier()\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Generating percolator pin files...\")\n",
    "global_out = postprocess.generate_percolator_input(pep_inds, psm_vals, spec_inds, pep_dataset, spec_dataset, \"target\")\n",
    "df = pd.DataFrame(global_out, columns=cols)\n",
    "df.sort_values(by=\"SNAP\", inplace=True, ascending=False)\n",
    "df.to_csv(join(out_pin_dir, \"target.pin\" if rank == 0 else \"decoy.pin\"), sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Wrote percolator files: \\n{}\".format(\n",
    "    join(out_pin_dir, \"target.pin\") if rank == 0 else join(out_pin_dir, \"decoy.pin\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from src.atlestrain import process\n",
    "from src.atlespredict import dbsearch, specdataset, pepdataset, preprocess, postprocess, specollate_model\n",
    "\n",
    "def ppm(val, ppm_val):\n",
    "    return (ppm_val / 1000000.0) * val\n",
    "\n",
    "def filtered_parallel_search(search_loader, peps, rank):\n",
    "    spec_inds = []\n",
    "    sort_inds = []\n",
    "    sort_vals = []\n",
    "\n",
    "    keep_psms = config.get_config(key=\"keep_psms\", section=\"search\")\n",
    "    precursor_tolerance = config.get_config(key=\"precursor_tolerance\", section=\"search\")\n",
    "\n",
    "    pbar = tqdm(search_loader, file=sys.stdout)\n",
    "    pbar.set_description('Running Database Search...')\n",
    "    # with progressbar.ProgressBar(max_value=len(search_loader)) as bar:\n",
    "    for idx, [spec_idx, spec_batch, spec_masses] in enumerate(pbar):\n",
    "        l_tol = precursor_tolerance\n",
    "        # min_mass = max(spec_masses[0] - l_tol, 0)\n",
    "        # max_mass = spec_masses[-1] + l_tol\n",
    "        min_mass = max(spec_masses[0] - ppm(spec_masses[0], l_tol), 0)\n",
    "        max_mass = spec_masses[-1] + ppm(spec_masses[-1], l_tol)\n",
    "\n",
    "        pep_min = pep_max = 0\n",
    "        while (pep_min < len(peps) and\n",
    "                min_mass - peps[pep_min][2] > 0.001):\n",
    "            pep_min += 1\n",
    "        while (pep_max < len(peps) and\n",
    "                max_mass - peps[pep_max][2] >= 0.001):\n",
    "            pep_max += 1\n",
    "\n",
    "        pep_batch = peps[pep_min:pep_max]\n",
    "        pep_masses = []\n",
    "\n",
    "        spec_batch = spec_batch.to(rank)\n",
    "        #print(\"pep batch len: {}\".format(len(pep_batch)))\n",
    "        l_pep_batch_size = 16384\n",
    "        # l_pep_batch_size = 32768\n",
    "        pep_loader = torch.utils.data.DataLoader(\n",
    "            dataset=pep_batch, batch_size=l_pep_batch_size, shuffle=False)\n",
    "        l_pep_dist = []\n",
    "        g_ids = []\n",
    "        for g_idx, l_pep_batch, l_pep_masses in pep_loader:\n",
    "            g_ids.extend(g_idx)\n",
    "            pep_masses.extend(l_pep_masses)\n",
    "            l_pep_batch = l_pep_batch.to(rank)\n",
    "            # spec_pep_mask = get_search_mask(spec_masses, l_pep_masses, precursor_tolerance).to(rank)\n",
    "            # spec_pep_mask[spec_pep_mask == 0] = float(\"inf\")\n",
    "            spec_pep_dist = 1.0 / process.pairwise_distances(spec_batch, l_pep_batch).to(\"cpu\")\n",
    "            l_pep_dist.append(spec_pep_dist)\n",
    "        g_ids = torch.IntTensor(g_ids)\n",
    "\n",
    "        if not l_pep_dist:\n",
    "            continue\n",
    "        pep_sort = torch.cat(l_pep_dist, 1)\n",
    "        spec_pep_mask = dbsearch.get_search_mask(spec_masses, pep_masses, precursor_tolerance)\n",
    "        pep_sort = (pep_sort * spec_pep_mask)\n",
    "        pep_sort = torch.cat((pep_sort, torch.zeros(len(spec_batch), keep_psms + 1)), axis=1)\n",
    "        pep_lcn = np.ma.masked_array(pep_sort, mask=pep_sort == 0).min(1).data\n",
    "        pep_sort = pep_sort.sort(descending=True, stable=True)\n",
    "        spec_inds.extend(spec_idx)\n",
    "        # no need to offset as g_ids is constructed for pep_batch.\n",
    "        sort_inds.append(g_ids[pep_sort.indices[:, :keep_psms + 1]])\n",
    "        sort_vals.append(torch.cat((pep_sort.values[:, :keep_psms + 1],\n",
    "                                    torch.from_numpy(pep_lcn).unsqueeze(1)), 1))\n",
    "\n",
    "        # bar.update(idx)\n",
    "    pep_inds = torch.cat(sort_inds, 0)\n",
    "    pep_vals = torch.cat(sort_vals, 0)\n",
    "    return spec_inds, pep_inds, pep_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = '9-0-1'\n",
    "spec_subset = spec_dataset.filt_dict[key]\n",
    "search_loader = torch.utils.data.DataLoader(\n",
    "    dataset=spec_subset, num_workers=0, batch_size=search_spec_batch_size, shuffle=False)\n",
    "\n",
    "for i in range(10):\n",
    "    print(spec_subset[i][2])\n",
    "l_spec_inds, l_pep_inds, l_psm_vals = filtered_parallel_search(search_loader, pep_dataset.filt_dict[key], rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.PARAM_PATH = join((dirname(__file__)), \"config.ini\")\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Num GPUs: {}\".format(num_gpus))\n",
    "mp.spawn(run_specollate_par, args=(2,), nprocs=2, join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(snap_model)\n",
    "model_ = model.Net().to(rank)\n",
    "model_ = nn.parallel.DistributedDataParallel(model_, device_ids=[rank])\n",
    "# model_.load_state_dict(torch.load('atles-out/16403437/models/pt-mass-ch-16403437-1toz70vi-472.pt')['model_state_dict'])\n",
    "# model_.load_state_dict(torch.load(\n",
    "#     '/lclhome/mtari008/DeepAtles/atles-out/123/models/pt-mass-ch-123-2zgb2ei9-385.pt')['model_state_dict'])\n",
    "model_.load_state_dict(torch.load(\n",
    "    '/lclhome/mtari008/DeepAtles/atles-out/1382/models/nist-massive-deepnovo-mass-ch-1382-c8mlqbq7-157.pt'\n",
    ")['model_state_dict'])\n",
    "model_ = model_.module\n",
    "count_parameters(model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('deepatles')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00151e0d58dec85c9023f7cf2cb52563442e50bc5bc18cc73647572af3f1d275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
